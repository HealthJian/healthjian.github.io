<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CUDAç¼–ç¨‹ä¸å¹¶è¡Œè®¡ç®— - HealthJiançš„åšå®¢</title>
    <!-- æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´CSSå¼•ç”¨è·¯å¾„ -->
    <link rel="stylesheet" href="../../../css/style.css">
    <link rel="stylesheet" href="../../../css/dark-mode.css">
    <link rel="stylesheet" href="../../../css/blog-post.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css">
    <link href="https://fonts.googleapis.com/css2?family=SF+Pro+Display:wght@300;400;500;600&display=swap" rel="stylesheet">
</head>
<body class="light-mode">
    <!-- å¯¼èˆªæ  -->
    <nav class="navbar">
        <div class="nav-left">
            <!-- æ ¹æ®å®é™…éƒ¨ç½²çš„æ–‡ä»¶å¤¹æ·±åº¦è°ƒæ•´è·¯å¾„ -->
            <a href="../../../index.html" class="logo">
                <img src="../../../images/githubherofigureimage.png" alt="Logo">
                <span class="site-name" data-en="HealthJian Blog" data-zh="HealthJian">HealthJian</span>
            </a>
        </div>
        <div class="nav-right">
            <ul class="menu">
                <!-- æ ¹æ®å®é™…éƒ¨ç½²çš„æ–‡ä»¶å¤¹æ·±åº¦è°ƒæ•´è·¯å¾„ -->
                <li><a href="../../../index.html" data-en="Home" data-zh="é¦–é¡µ">é¦–é¡µ</a></li>
                <li><a href="../../../blog.html" data-en="Blog" data-zh="åšå®¢">åšå®¢</a></li>
                <li><a href="https://github.com/CaiNiaojian" target="_blank" data-en="GitHub" data-zh="GitHub">GitHub</a></li>
                <li><a href="../../../links.html" data-en="Links" data-zh="é“¾æ¥">é“¾æ¥</a></li>
                <li><a href="../../../about.html" data-en="About" data-zh="å…³äº">å…³äº</a></li>
            </ul>
            <div class="social-icons">
                <a href="https://steamcommunity.com/id/yoursteamid" target="_blank" title="Steam"><i class="fab fa-steam"></i></a>
                <a href="mailto:gaojian1573@foxmail.com" title="Email"><i class="fas fa-envelope"></i></a>
            </div>
            <button id="theme-toggle" class="theme-toggle" title="åˆ‡æ¢ä¸»é¢˜">
                <i class="fas fa-moon"></i>
                <i class="fas fa-sun"></i>
            </button>
            <button id="lang-toggle" class="lang-toggle" title="åˆ‡æ¢è¯­è¨€">
                <span data-en="EN" data-zh="ä¸­">ä¸­</span>
            </button>
        </div>
    </nav>

    <div class="blog-post-container">
        <!-- ä¾§è¾¹æ  -->
        <aside class="blog-sidebar">
            <div class="toc-container">
                <h3 data-en="Table of Contents" data-zh="ç›®å½•">ç›®å½•</h3>
                <ul class="toc-list">
                    <!-- æ–‡ç« ç›®å½•ï¼Œæ ¹æ®å®é™…å†…å®¹æ·»åŠ æˆ–ä¿®æ”¹ -->
                    <li><a href="#section-1" data-en="I. Introduction" data-zh="ä¸€ã€å¼•è¨€">ä¸€ã€å¼•è¨€</a></li>
                    <li><a href="#section-2" data-en="II. Multi-threading Programming" data-zh="äºŒã€å¤šçº¿ç¨‹ç¼–ç¨‹">äºŒã€å¤šçº¿ç¨‹ç¼–ç¨‹</a></li>
                    <li><a href="#section-3" data-en="III. CUDA Programming Basics" data-zh="ä¸‰ã€CUDAç¼–ç¨‹åŸºç¡€">ä¸‰ã€CUDAç¼–ç¨‹åŸºç¡€</a></li>
                    <li><a href="#section-4" data-en="IV. Advanced CUDA Techniques" data-zh="å››ã€CUDAé«˜çº§æŠ€æœ¯">å››ã€CUDAé«˜çº§æŠ€æœ¯</a></li>
                    <!-- æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šç« èŠ‚ -->
                </ul>
            </div>
            
            <div class="post-meta-info">
                <div class="post-date">
                    <i class="far fa-calendar-alt"></i>
                    <span>2025-05-26</span> <!-- æ›´æ–°å®é™…å‘å¸ƒæ—¥æœŸ -->
                </div>
                <div class="post-tags">
                    <i class="fas fa-tags"></i>
                    <!-- æ ¹æ®æ–‡ç« ä¸»é¢˜ä¿®æ”¹æ ‡ç­¾ -->
                    <span class="tag" data-en="CUDA" data-zh="CUDA">CUDA</span>
                    <span class="tag" data-en="Parallel Computing" data-zh="å¹¶è¡Œè®¡ç®—">å¹¶è¡Œè®¡ç®—</span>
                    <span class="tag" data-en="Programming" data-zh="ç¼–ç¨‹">ç¼–ç¨‹</span>
                </div>
                <div class="post-category">
                    <i class="fas fa-folder"></i>
                    <!-- ä¿®æ”¹ä¸ºå®é™…åˆ†ç±» -->
                    <span data-en="Technology" data-zh="æŠ€æœ¯">æŠ€æœ¯</span>
                </div>
            </div>
            
            <div class="post-navigation">
                <h3 data-en="Navigation" data-zh="å¯¼èˆª">å¯¼èˆª</h3>
                <div class="nav-buttons">
                    <a href="../../../blog.html" class="nav-button" data-en="Back to Blog" data-zh="è¿”å›åšå®¢åˆ—è¡¨">
                        <i class="fas fa-arrow-left"></i>
                        <span data-en="Back to Blog" data-zh="è¿”å›åšå®¢åˆ—è¡¨">è¿”å›åšå®¢åˆ—è¡¨</span>
                    </a>
                </div>
            </div>
        </aside>

        <!-- æ–‡ç« ä¸»ä½“ -->
        <main class="blog-post-main">
            <article class="blog-post-content">
                <header class="post-header">
                    <h1 class="post-title">
                        <div class="bilingual-content">
                            <span class="zh-content">CUDAç¼–ç¨‹ä¸å¹¶è¡Œè®¡ç®—</span>
                            <span class="en-content" style="display: none;">CUDA Programming and Parallel Computing</span>
                        </div>
                    </h1>
                </header>
                
                <div class="post-body">
                    <!-- ç¬¬ä¸€éƒ¨åˆ†ï¼šå¼•è¨€ -->
                    <section id="section-1">
                        <h2 data-en="I. Introduction" data-zh="ä¸€ã€å¼•è¨€">ä¸€ã€å¼•è¨€</h2>
                        
                        <div class="bilingual-content">
                            <p class="zh-content">éšç€è®¡ç®—æœºç¡¬ä»¶æŠ€æœ¯çš„å‘å±•ï¼Œå¤šæ ¸å¤„ç†å™¨å’ŒGPUå·²ç»æˆä¸ºä¸»æµè®¡ç®—è®¾å¤‡ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™äº›ç¡¬ä»¶èµ„æºï¼Œå¹¶è¡Œè®¡ç®—æŠ€æœ¯å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æœ¬æ–‡å°†ä»‹ç»å¤šçº¿ç¨‹ç¼–ç¨‹å’ŒCUDAç¼–ç¨‹æŠ€æœ¯ï¼Œè¿™ä¸¤ç§æŠ€æœ¯æ˜¯ç°ä»£é«˜æ€§èƒ½è®¡ç®—çš„åŸºç¡€ã€‚</p>
                            
                            <p class="en-content" style="display: none;">With the development of computer hardware technology, multi-core processors and GPUs have become mainstream computing devices. To fully utilize these hardware resources, parallel computing technology has become increasingly important. This article will introduce multi-threading programming and CUDA programming techniques, which are the foundation of modern high-performance computing.</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content">å¤šçº¿ç¨‹ç¼–ç¨‹å…è®¸ç¨‹åºåŒæ—¶æ‰§è¡Œå¤šä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹å¯ä»¥ç‹¬ç«‹å¤„ç†ä¸åŒçš„ä»»åŠ¡ï¼Œä»è€Œæé«˜ç¨‹åºçš„æ‰§è¡Œæ•ˆç‡ã€‚è€ŒCUDAï¼ˆCompute Unified Device Architectureï¼‰æ˜¯NVIDIAå…¬å¸å¼€å‘çš„å¹¶è¡Œè®¡ç®—å¹³å°å’Œç¼–ç¨‹æ¨¡å‹ï¼Œå®ƒå…è®¸å¼€å‘è€…åˆ©ç”¨NVIDIA GPUçš„å¼ºå¤§è®¡ç®—èƒ½åŠ›æ¥åŠ é€Ÿè®¡ç®—å¯†é›†å‹åº”ç”¨ã€‚</p>
                            
                            <p class="en-content" style="display: none;">Multi-threading programming allows programs to execute multiple threads simultaneously, with each thread independently processing different tasks, thereby improving program execution efficiency. CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA, which allows developers to leverage the powerful computing capabilities of NVIDIA GPUs to accelerate compute-intensive applications.</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content">æœ¬æ–‡å°†é€šè¿‡å…·ä½“çš„ä»£ç ç¤ºä¾‹ï¼Œè¯¦ç»†è®²è§£å¤šçº¿ç¨‹ç¼–ç¨‹å’ŒCUDAç¼–ç¨‹çš„åŸºæœ¬æ¦‚å¿µã€å®ç°æ–¹æ³•å’Œä¼˜åŒ–æŠ€å·§ï¼Œå¸®åŠ©è¯»è€…æŒæ¡è¿™äº›é‡è¦çš„å¹¶è¡Œè®¡ç®—æŠ€æœ¯ã€‚</p>
                            
                            <p class="en-content" style="display: none;">This article will use specific code examples to explain in detail the basic concepts, implementation methods, and optimization techniques of multi-threading programming and CUDA programming, helping readers master these important parallel computing technologies.</p>
                        </div>
                    </section>
                    
                    <!-- ç¬¬äºŒéƒ¨åˆ† -->
                    <section id="section-2">
                        <h2 data-en="II. Multi-threading Programming" data-zh="äºŒã€å¤šçº¿ç¨‹ç¼–ç¨‹">äºŒã€å¤šçº¿ç¨‹ç¼–ç¨‹</h2>
                        
                        <h3 data-en="2.1 Introduction to Multi-threading" data-zh="2.1 å¤šçº¿ç¨‹ç¼–ç¨‹ç®€ä»‹">2.1 å¤šçº¿ç¨‹ç¼–ç¨‹ç®€ä»‹</h3>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>å¤šçº¿ç¨‹çš„åŸºæœ¬æ¦‚å¿µ</strong><br>
                            å¤šçº¿ç¨‹ç¼–ç¨‹æ˜¯ä¸€ç§å¹¶è¡Œè®¡ç®—æŠ€æœ¯ï¼Œå®ƒå…è®¸ç¨‹åºåŒæ—¶æ‰§è¡Œå¤šä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹å¯ä»¥ç‹¬ç«‹å¤„ç†ä¸åŒçš„ä»»åŠ¡ã€‚åœ¨å¤šæ ¸å¤„ç†å™¨ä¸Šï¼Œå¤šçº¿ç¨‹å¯ä»¥å……åˆ†åˆ©ç”¨å¤šä¸ªCPUæ ¸å¿ƒï¼Œæ˜¾è‘—æé«˜ç¨‹åºçš„æ‰§è¡Œæ•ˆç‡ã€‚</p>
                            
                            <p class="en-content" style="display: none;"><strong>Basic Concepts of Multi-threading</strong><br>
                            Multi-threading programming is a parallel computing technique that allows programs to execute multiple threads simultaneously, with each thread independently processing different tasks. On multi-core processors, multi-threading can fully utilize multiple CPU cores, significantly improving program execution efficiency.</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>å¤šçº¿ç¨‹çš„ä¼˜åŠ¿</strong></p>
                            <ul class="zh-content">
                                <li>æé«˜ç¨‹åºå“åº”æ€§ï¼šè€—æ—¶æ“ä½œå¯ä»¥åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œä¿æŒç”¨æˆ·ç•Œé¢çš„å“åº”</li>
                                <li>æé«˜èµ„æºåˆ©ç”¨ç‡ï¼šå……åˆ†åˆ©ç”¨å¤šæ ¸CPUçš„è®¡ç®—èƒ½åŠ›</li>
                                <li>ç®€åŒ–ç¨‹åºè®¾è®¡ï¼šå¯ä»¥å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¤šä¸ªç®€å•ä»»åŠ¡å¹¶è¡Œå¤„ç†</li>
                                <li>æé«˜æ‰§è¡Œæ•ˆç‡ï¼šé€‚å½“çš„å¹¶è¡ŒåŒ–å¯ä»¥æ˜¾è‘—å‡å°‘ç¨‹åºçš„æ‰§è¡Œæ—¶é—´</li>
                            </ul>
                            
                            <p class="en-content" style="display: none;"><strong>Advantages of Multi-threading</strong></p>
                            <ul class="en-content" style="display: none;">
                                <li>Improved program responsiveness: Time-consuming operations can be executed in background threads, keeping the user interface responsive</li>
                                <li>Improved resource utilization: Fully utilizing the computing power of multi-core CPUs</li>
                                <li>Simplified program design: Complex tasks can be broken down into multiple simple tasks for parallel processing</li>
                                <li>Improved execution efficiency: Appropriate parallelization can significantly reduce program execution time</li>
                            </ul>
                        </div>
                        
                        <h3 data-en="2.2 Pthread Programming Example" data-zh="2.2 Pthreadç¼–ç¨‹ç¤ºä¾‹">2.2 Pthreadç¼–ç¨‹ç¤ºä¾‹</h3>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>POSIXçº¿ç¨‹åº“ï¼ˆPthreadï¼‰</strong><br>
                            Pthreadæ˜¯POSIXçº¿ç¨‹æ ‡å‡†å®šä¹‰çš„çº¿ç¨‹APIï¼Œå®ƒæä¾›äº†ä¸€ç»„åˆ›å»ºå’Œç®¡ç†çº¿ç¨‹çš„å‡½æ•°ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨Pthreadè®¡ç®—Ï€å€¼çš„å¹¶è¡Œç¨‹åºç¤ºä¾‹ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>POSIX Thread Library (Pthread)</strong><br>
                            Pthread is a thread API defined by the POSIX thread standard, which provides a set of functions for creating and managing threads. Below is an example of a parallel program using Pthread to calculate the value of Ï€:</p>
                        </div>
                        
                        <!-- ä»£ç ç¤ºä¾‹ -->
                        <div class="code-block">
                            <pre><code class="language-cpp">#include &lt;pthread.h&gt;

const long long n = 10000000000;
const int thread_count = 10;
double sum = 0.0;
pthread_mutex_t mutexsum;

void* Thread_sum(void* rank);

int main() {
    pthread_mutex_init(&mutexsum, NULL);
    
    pthread_t thread_ID[thread_count];
    int value[thread_count];
    
    // Initialize thread values
    for (int i = 0; i < thread_count; i++) {
        value[i] = i;
    }

    // Create the threads
    for (int i = 0; i < thread_count; i++) {
        pthread_create(&thread_ID[i], NULL, Thread_sum, &value[i]);
    }
    
    // Wait for threads to terminate
    for (int i = 0; i < thread_count; i++) {
        pthread_join(thread_ID[i], NULL);
    }
    
    pthread_mutex_destroy(&mutexsum);
    pthread_exit(NULL);
    return 0;
}

void* Thread_sum(void* rank) {
    int my_rank = *(int*)rank;
    double my_sum = 0.0;
    long long my_n = n / thread_count;
    long long my_first_i = my_n * my_rank;
    long long my_last_i = my_first_i + my_n;
    
    // Calculate partial sum
    for (long long i = my_first_i; i < my_last_i; i++) {
        double temp = (i + 0.5) / n;
        my_sum += 4.0 / (1.0 + temp * temp) / n;
    }
  
    // Add to global sum
    pthread_mutex_lock(&mutexsum);
    sum += my_sum;
    pthread_mutex_unlock(&mutexsum);
    
    pthread_exit(NULL);
    return NULL;
}</code></pre>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>ä»£ç åˆ†æ</strong><br>
                            ä¸Šè¿°ä»£ç ä½¿ç”¨å¤šçº¿ç¨‹è®¡ç®—Ï€çš„è¿‘ä¼¼å€¼ï¼Œé‡‡ç”¨æ•°å€¼ç§¯åˆ†æ–¹æ³•ã€‚ä¸»è¦å®ç°æ­¥éª¤å¦‚ä¸‹ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>Code Analysis</strong><br>
                            The above code uses multi-threading to calculate an approximation of Ï€ using numerical integration. The main implementation steps are as follows:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ol class="zh-content">
                                <li><strong>åˆå§‹åŒ–äº’æ–¥é”</strong>ï¼šä½¿ç”¨<code>pthread_mutex_init</code>åˆå§‹åŒ–äº’æ–¥é”ï¼Œç”¨äºä¿æŠ¤å…±äº«å˜é‡<code>sum</code>ã€‚</li>
                                <li><strong>åˆ›å»ºçº¿ç¨‹</strong>ï¼šä½¿ç”¨<code>pthread_create</code>åˆ›å»º10ä¸ªå·¥ä½œçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹è´Ÿè´£è®¡ç®—ä¸€éƒ¨åˆ†ç§¯åˆ†ã€‚</li>
                                <li><strong>ç­‰å¾…çº¿ç¨‹å®Œæˆ</strong>ï¼šä½¿ç”¨<code>pthread_join</code>ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆè®¡ç®—ã€‚</li>
                                <li><strong>çº¿ç¨‹å‡½æ•°å®ç°</strong>ï¼šæ¯ä¸ªçº¿ç¨‹è®¡ç®—è‡ªå·±è´Ÿè´£çš„åŒºé—´å†…çš„éƒ¨åˆ†å’Œï¼Œç„¶åä½¿ç”¨äº’æ–¥é”å®‰å…¨åœ°å°†ç»“æœç´¯åŠ åˆ°å…¨å±€å˜é‡<code>sum</code>ä¸­ã€‚</li>
                                <li><strong>æ•°å€¼ç§¯åˆ†æ–¹æ³•</strong>ï¼šä½¿ç”¨ä¸­ç‚¹æ³•è®¡ç®—å‡½æ•°<code>f(x) = 4/(1+xÂ²)</code>åœ¨[0,1]åŒºé—´ä¸Šçš„ç§¯åˆ†ï¼Œè¯¥ç§¯åˆ†çš„ç²¾ç¡®å€¼ä¸ºÏ€ã€‚</li>
                            </ol>
                            
                            <ol class="en-content" style="display: none;">
                                <li><strong>Initialize Mutex</strong>: Use <code>pthread_mutex_init</code> to initialize a mutex for protecting the shared variable <code>sum</code>.</li>
                                <li><strong>Create Threads</strong>: Use <code>pthread_create</code> to create 10 worker threads, each responsible for calculating a portion of the integral.</li>
                                <li><strong>Wait for Thread Completion</strong>: Use <code>pthread_join</code> to wait for all threads to complete their calculations.</li>
                                <li><strong>Thread Function Implementation</strong>: Each thread calculates the partial sum within its responsible interval, then safely adds the result to the global variable <code>sum</code> using a mutex.</li>
                                <li><strong>Numerical Integration Method</strong>: Use the midpoint method to calculate the integral of the function <code>f(x) = 4/(1+xÂ²)</code> over the interval [0,1], the exact value of which is Ï€.</li>
                            </ol>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>å…³é”®å¹¶è¡Œç¼–ç¨‹æ¦‚å¿µ</strong></p>
                            <ul class="zh-content">
                                <li><strong>ä»»åŠ¡åˆ†è§£</strong>ï¼šå°†å¤§ä»»åŠ¡ï¼ˆè®¡ç®—Ï€ï¼‰åˆ†è§£ä¸ºå¤šä¸ªå°ä»»åŠ¡ï¼ˆè®¡ç®—éƒ¨åˆ†ç§¯åˆ†ï¼‰ï¼Œç”±ä¸åŒçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œã€‚</li>
                                <li><strong>è´Ÿè½½å‡è¡¡</strong>ï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†ç›¸åŒæ•°é‡çš„è¿­ä»£ï¼Œç¡®ä¿å·¥ä½œè´Ÿè½½å‡åŒ€åˆ†å¸ƒã€‚</li>
                                <li><strong>äº’æ–¥é”</strong>ï¼šä½¿ç”¨äº’æ–¥é”ä¿æŠ¤å…±äº«èµ„æºï¼ˆå…¨å±€å˜é‡<code>sum</code>ï¼‰ï¼Œé˜²æ­¢æ•°æ®ç«äº‰ã€‚</li>
                                <li><strong>çº¿ç¨‹åŒæ­¥</strong>ï¼šä½¿ç”¨<code>pthread_join</code>ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆï¼Œç¡®ä¿ç»“æœçš„æ­£ç¡®æ€§ã€‚</li>
                            </ul>
                            
                            <ul class="en-content" style="display: none;">
                                <li><strong>Task Decomposition</strong>: Breaking down the large task (calculating Ï€) into multiple smaller tasks (calculating partial integrals) to be executed in parallel by different threads.</li>
                                <li><strong>Load Balancing</strong>: Each thread processes the same number of iterations, ensuring an even distribution of workload.</li>
                                <li><strong>Mutex</strong>: Using a mutex to protect shared resources (global variable <code>sum</code>), preventing data races.</li>
                                <li><strong>Thread Synchronization</strong>: Using <code>pthread_join</code> to wait for all threads to complete, ensuring the correctness of the result.</li>
                            </ul>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>æ€§èƒ½è€ƒè™‘</strong><br>
                            è™½ç„¶å¤šçº¿ç¨‹å¯ä»¥æé«˜è®¡ç®—æ•ˆç‡ï¼Œä½†ä¹Ÿå­˜åœ¨ä¸€äº›æ€§èƒ½ç“¶é¢ˆï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>Performance Considerations</strong><br>
                            Although multi-threading can improve computational efficiency, there are also some performance bottlenecks:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ul class="zh-content">
                                <li><strong>çº¿ç¨‹åˆ›å»ºå¼€é”€</strong>ï¼šåˆ›å»ºå’Œç®¡ç†çº¿ç¨‹éœ€è¦é¢å¤–çš„ç³»ç»Ÿèµ„æºã€‚</li>
                                <li><strong>äº’æ–¥é”å¼€é”€</strong>ï¼šé¢‘ç¹çš„é”æ“ä½œä¼šå¯¼è‡´çº¿ç¨‹ç­‰å¾…ï¼Œé™ä½å¹¶è¡Œæ•ˆç‡ã€‚</li>
                                <li><strong>ç¼“å­˜ä¸€è‡´æ€§</strong>ï¼šå¤šæ ¸å¤„ç†å™¨ä¸Šçš„ç¼“å­˜åŒæ­¥å¯èƒ½æˆä¸ºæ€§èƒ½ç“¶é¢ˆã€‚</li>
                                <li><strong>çº¿ç¨‹æ•°é‡é€‰æ‹©</strong>ï¼šçº¿ç¨‹æ•°é‡åº”ä¸CPUæ ¸å¿ƒæ•°ç›¸åŒ¹é…ï¼Œè¿‡å¤šçš„çº¿ç¨‹ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡åˆ‡æ¢å¼€é”€å¢åŠ ã€‚</li>
                            </ul>
                            
                            <ul class="en-content" style="display: none;">
                                <li><strong>Thread Creation Overhead</strong>: Creating and managing threads requires additional system resources.</li>
                                <li><strong>Mutex Overhead</strong>: Frequent lock operations can cause thread waiting, reducing parallel efficiency.</li>
                                <li><strong>Cache Coherence</strong>: Cache synchronization on multi-core processors can become a performance bottleneck.</li>
                                <li><strong>Thread Count Selection</strong>: The number of threads should match the number of CPU cores; too many threads will increase context switching overhead.</li>
                            </ul>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>ä¼˜åŒ–ç­–ç•¥</strong><br>
                            é’ˆå¯¹ä¸Šè¿°æ€§èƒ½é—®é¢˜ï¼Œå¯ä»¥é‡‡å–ä»¥ä¸‹ä¼˜åŒ–ç­–ç•¥ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>Optimization Strategies</strong><br>
                            For the performance issues mentioned above, the following optimization strategies can be adopted:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ul class="zh-content">
                                <li><strong>å‡å°‘é”ç«äº‰</strong>ï¼šæ¯ä¸ªçº¿ç¨‹å…ˆåœ¨æœ¬åœ°ç´¯åŠ ç»“æœï¼Œæœ€åæ‰æ›´æ–°å…¨å±€å˜é‡ï¼Œå‡å°‘äº’æ–¥é”çš„ä½¿ç”¨é¢‘ç‡ã€‚</li>
                                <li><strong>ä½¿ç”¨çº¿ç¨‹æ± </strong>ï¼šé¢„å…ˆåˆ›å»ºçº¿ç¨‹æ± ï¼Œé¿å…é¢‘ç¹åˆ›å»ºå’Œé”€æ¯çº¿ç¨‹çš„å¼€é”€ã€‚</li>
                                <li><strong>æ•°æ®å±€éƒ¨æ€§ä¼˜åŒ–</strong>ï¼šåˆç†å®‰æ’æ•°æ®è®¿é—®æ¨¡å¼ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡ã€‚</li>
                                <li><strong>åŠ¨æ€è´Ÿè½½å‡è¡¡</strong>ï¼šå¯¹äºä¸å‡åŒ€çš„å·¥ä½œè´Ÿè½½ï¼Œå¯ä»¥ä½¿ç”¨å·¥ä½œçªƒå–ç­‰æŠ€æœ¯å®ç°åŠ¨æ€è´Ÿè½½å‡è¡¡ã€‚</li>
                            </ul>
                            
                            <ul class="en-content" style="display: none;">
                                <li><strong>Reduce Lock Contention</strong>: Each thread accumulates results locally first, then updates the global variable at the end, reducing the frequency of mutex use.</li>
                                <li><strong>Use Thread Pools</strong>: Create thread pools in advance to avoid the overhead of frequent thread creation and destruction.</li>
                                <li><strong>Data Locality Optimization</strong>: Arrange data access patterns reasonably to improve cache hit rates.</li>
                                <li><strong>Dynamic Load Balancing</strong>: For uneven workloads, techniques such as work stealing can be used to achieve dynamic load balancing.</li>
                            </ul>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>æ€»ç»“</strong><br>
                            å¤šçº¿ç¨‹ç¼–ç¨‹æ˜¯ä¸€ç§å¼ºå¤§çš„å¹¶è¡Œè®¡ç®—æŠ€æœ¯ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜ç¨‹åºçš„æ‰§è¡Œæ•ˆç‡ã€‚é€šè¿‡åˆç†çš„ä»»åŠ¡åˆ†è§£ã€è´Ÿè½½å‡è¡¡å’ŒåŒæ­¥æœºåˆ¶ï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨å¤šæ ¸å¤„ç†å™¨çš„è®¡ç®—èƒ½åŠ›ã€‚ä½†åŒæ—¶ï¼Œä¹Ÿéœ€è¦æ³¨æ„çº¿ç¨‹ç®¡ç†ã€äº’æ–¥é”å’Œç¼“å­˜ä¸€è‡´æ€§ç­‰æ–¹é¢çš„æ€§èƒ½å¼€é”€ï¼Œé‡‡å–é€‚å½“çš„ä¼˜åŒ–ç­–ç•¥ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚<span class="emoji">ğŸ“</span></p>
                            
                            <p class="en-content" style="display: none;"><strong>Summary</strong><br>
                            Multi-threading programming is a powerful parallel computing technique that can effectively improve program execution efficiency. Through reasonable task decomposition, load balancing, and synchronization mechanisms, the computing power of multi-core processors can be fully utilized. However, attention should also be paid to the performance overhead of thread management, mutexes, and cache coherence, and appropriate optimization strategies should be adopted to achieve optimal performance.<span class="emoji">ğŸ“</span></p>
                        </div>
                    </section>
                    
                    <!-- ç¬¬ä¸‰éƒ¨åˆ† -->
                    <section id="section-3">
                        <h2 data-en="III. CUDA Programming Basics" data-zh="ä¸‰ã€CUDAç¼–ç¨‹åŸºç¡€">ä¸‰ã€CUDAç¼–ç¨‹åŸºç¡€</h2>
                        
                        <h3 data-en="3.1 Introduction to CUDA" data-zh="3.1 CUDAç®€ä»‹">3.1 CUDAç®€ä»‹</h3>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>ä»€ä¹ˆæ˜¯CUDA</strong><br>
                            CUDAï¼ˆCompute Unified Device Architectureï¼‰æ˜¯NVIDIAå…¬å¸å¼€å‘çš„å¹¶è¡Œè®¡ç®—å¹³å°å’Œç¼–ç¨‹æ¨¡å‹ï¼Œå®ƒå…è®¸å¼€å‘è€…åˆ©ç”¨NVIDIA GPUçš„å¼ºå¤§è®¡ç®—èƒ½åŠ›æ¥åŠ é€Ÿè®¡ç®—å¯†é›†å‹åº”ç”¨ã€‚CUDAæä¾›äº†C/C++è¯­è¨€æ‰©å±•å’ŒAPIï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿç¼–å†™åœ¨GPUä¸Šæ‰§è¡Œçš„å¹¶è¡Œç¨‹åºã€‚</p>
                            
                            <p class="en-content" style="display: none;"><strong>What is CUDA</strong><br>
                            CUDA (Compute Unified Device Architecture) is a parallel computing platform and programming model developed by NVIDIA that allows developers to utilize the powerful computing capabilities of NVIDIA GPUs to accelerate compute-intensive applications. CUDA provides C/C++ language extensions and APIs that enable developers to write parallel programs that run on GPUs.</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>CUDAçš„ä¼˜åŠ¿</strong></p>
                            <ul class="zh-content">
                                <li>é«˜åº¦å¹¶è¡Œï¼šç°ä»£GPUæ‹¥æœ‰æ•°åƒä¸ªæ ¸å¿ƒï¼Œå¯ä»¥åŒæ—¶æ‰§è¡Œå¤§é‡çº¿ç¨‹</li>
                                <li>é«˜å†…å­˜å¸¦å®½ï¼šGPUå†…å­˜å¸¦å®½é€šå¸¸æ¯”CPUé«˜5-10å€</li>
                                <li>ä¸“ä¸ºå¹¶è¡Œè®¡ç®—ä¼˜åŒ–ï¼šGPUæ¶æ„ä¸“ä¸ºæ•°æ®å¹¶è¡Œä»»åŠ¡è®¾è®¡</li>
                                <li>æ˜“äºç¼–ç¨‹ï¼šCUDAæä¾›äº†åŸºäºC/C++çš„ç¼–ç¨‹æ¥å£ï¼Œé™ä½äº†å­¦ä¹ é—¨æ§›</li>
                            </ul>
                            
                            <p class="en-content" style="display: none;"><strong>Advantages of CUDA</strong></p>
                            <ul class="en-content" style="display: none;">
                                <li>Highly Parallel: Modern GPUs have thousands of cores that can execute a large number of threads simultaneously</li>
                                <li>High Memory Bandwidth: GPU memory bandwidth is typically 5-10 times higher than CPU</li>
                                <li>Optimized for Parallel Computing: GPU architecture is designed for data-parallel tasks</li>
                                <li>Easy to Program: CUDA provides a C/C++-based programming interface, lowering the learning curve</li>
                            </ul>
                        </div>
                        
                        <h3 data-en="3.2 CUDA Architecture" data-zh="3.2 CUDAæ¶æ„">3.2 CUDAæ¶æ„</h3>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>CUDAç¼–ç¨‹æ¨¡å‹</strong><br>
                            CUDAç¼–ç¨‹æ¨¡å‹åŸºäºå¼‚æ„è®¡ç®—çš„æ¦‚å¿µï¼Œå…¶ä¸­CPUï¼ˆä¸»æœºï¼‰å’ŒGPUï¼ˆè®¾å¤‡ï¼‰ååŒå·¥ä½œã€‚ç¨‹åºçš„ä¸²è¡Œéƒ¨åˆ†åœ¨CPUä¸Šæ‰§è¡Œï¼Œè€Œå¹¶è¡Œéƒ¨åˆ†åˆ™åœ¨GPUä¸Šæ‰§è¡Œã€‚CUDAç¨‹åºçš„å…¸å‹æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>CUDA Programming Model</strong><br>
                            The CUDA programming model is based on the concept of heterogeneous computing, where the CPU (host) and GPU (device) work together. The serial part of the program is executed on the CPU, while the parallel part is executed on the GPU. The typical execution flow of a CUDA program is as follows:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ol class="zh-content">
                                <li>åœ¨ä¸»æœºï¼ˆCPUï¼‰ä¸Šåˆ†é…å†…å­˜</li>
                                <li>å°†æ•°æ®ä»ä¸»æœºå†…å­˜å¤åˆ¶åˆ°è®¾å¤‡ï¼ˆGPUï¼‰å†…å­˜</li>
                                <li>åœ¨è®¾å¤‡ä¸Šæ‰§è¡Œå¹¶è¡Œè®¡ç®—ï¼ˆé€šè¿‡å¯åŠ¨CUDAæ ¸å‡½æ•°ï¼‰</li>
                                <li>å°†ç»“æœä»è®¾å¤‡å†…å­˜å¤åˆ¶å›ä¸»æœºå†…å­˜</li>
                                <li>é‡Šæ”¾è®¾å¤‡å’Œä¸»æœºå†…å­˜</li>
                            </ol>
                            
                            <ol class="en-content" style="display: none;">
                                <li>Allocate memory on the host (CPU)</li>
                                <li>Copy data from host memory to device (GPU) memory</li>
                                <li>Execute parallel computation on the device (by launching CUDA kernels)</li>
                                <li>Copy results from device memory back to host memory</li>
                                <li>Free device and host memory</li>
                            </ol>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>CUDAçº¿ç¨‹å±‚æ¬¡ç»“æ„</strong><br>
                            CUDAä½¿ç”¨ä¸€ç§å±‚æ¬¡åŒ–çš„çº¿ç¨‹ç»„ç»‡ç»“æ„ï¼ŒåŒ…æ‹¬çº¿ç¨‹ï¼ˆThreadï¼‰ã€çº¿ç¨‹å—ï¼ˆBlockï¼‰å’Œç½‘æ ¼ï¼ˆGridï¼‰ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>CUDA Thread Hierarchy</strong><br>
                            CUDA uses a hierarchical thread organization structure, including Threads, Blocks, and Grids:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ul class="zh-content">
                                <li><strong>çº¿ç¨‹ï¼ˆThreadï¼‰</strong>ï¼šæœ€åŸºæœ¬çš„æ‰§è¡Œå•å…ƒï¼Œæ¯ä¸ªçº¿ç¨‹æ‰§è¡Œç›¸åŒçš„æ ¸å‡½æ•°ä»£ç </li>
                                <li><strong>çº¿ç¨‹å—ï¼ˆBlockï¼‰</strong>ï¼šç”±å¤šä¸ªçº¿ç¨‹ç»„æˆï¼ŒåŒä¸€å—å†…çš„çº¿ç¨‹å¯ä»¥é€šè¿‡å…±äº«å†…å­˜é€šä¿¡å’ŒåŒæ­¥</li>
                                <li><strong>ç½‘æ ¼ï¼ˆGridï¼‰</strong>ï¼šç”±å¤šä¸ªçº¿ç¨‹å—ç»„æˆï¼Œä»£è¡¨æ•´ä¸ªå¹¶è¡Œä»»åŠ¡</li>
                            </ul>
                            
                            <ul class="en-content" style="display: none;">
                                <li><strong>Thread</strong>: The most basic execution unit, each thread executes the same kernel function code</li>
                                <li><strong>Block</strong>: Composed of multiple threads, threads within the same block can communicate and synchronize through shared memory</li>
                                <li><strong>Grid</strong>: Composed of multiple blocks, representing the entire parallel task</li>
                            </ul>
                        </div>
                        
                        <h3 data-en="3.3 Basic CUDA Program Example" data-zh="3.3 åŸºæœ¬CUDAç¨‹åºç¤ºä¾‹">3.3 åŸºæœ¬CUDAç¨‹åºç¤ºä¾‹</h3>
                        
                        <div class="bilingual-content">
                            <p class="zh-content">ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„CUDAç¨‹åºç¤ºä¾‹ï¼Œç”¨äºè®¡ç®—ä¸¤ä¸ªå‘é‡çš„åŠ æ³•ï¼š</p>
                            
                            <p class="en-content" style="display: none;">Below is a simple CUDA program example for calculating the addition of two vectors:</p>
                        </div>
                        
                        <div class="code-block">
                            <pre><code class="language-cpp">#include &lt;stdio.h&gt;
                    #include &lt;cuda_runtime.h&gt;
                    
                    // CUDAæ ¸å‡½æ•°ï¼šå‘é‡åŠ æ³•
                    __global__ void vectorAdd(const float *A, const float *B, float *C, int numElements) {
                        int i = blockDim.x * blockIdx.x + threadIdx.x;
                        if (i < numElements) {
                            C[i] = A[i] + B[i];
                        }
                    }
                    
                    int main(void) {
                        // å‘é‡å¤§å°
                        int numElements = 50000;
                        size_t size = numElements * sizeof(float);
                        
                        // ä¸»æœºå†…å­˜åˆ†é…
                        float *h_A = (float *)malloc(size);
                        float *h_B = (float *)malloc(size);
                        float *h_C = (float *)malloc(size);
                        
                        // åˆå§‹åŒ–è¾“å…¥å‘é‡
                        for (int i = 0; i < numElements; ++i) {
                            h_A[i] = rand()/(float)RAND_MAX;
                            h_B[i] = rand()/(float)RAND_MAX;
                        }
                        
                        // è®¾å¤‡å†…å­˜åˆ†é…
                        float *d_A = NULL;
                        float *d_B = NULL;
                        float *d_C = NULL;
                        cudaMalloc((void **)&d_A, size);
                        cudaMalloc((void **)&d_B, size);
                        cudaMalloc((void **)&d_C, size);
                        
                        // å°†æ•°æ®ä»ä¸»æœºå¤åˆ¶åˆ°è®¾å¤‡
                        cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
                        cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);
                        
                        // å¯åŠ¨CUDAæ ¸å‡½æ•°
                        int threadsPerBlock = 256;
                        int blocksPerGrid = (numElements + threadsPerBlock - 1) / threadsPerBlock;
                        vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, numElements);
                        
                        // å°†ç»“æœä»è®¾å¤‡å¤åˆ¶å›ä¸»æœº
                        cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);
                        
                        // éªŒè¯ç»“æœ
                        for (int i = 0; i < numElements; ++i) {
                            if (fabs(h_A[i] + h_B[i] - h_C[i]) > 1e-5) {
                                fprintf(stderr, "Result verification failed at element %d!\n", i);
                                exit(EXIT_FAILURE);
                            }
                        }
                        printf("Test PASSED\n");
                        
                        // é‡Šæ”¾è®¾å¤‡å†…å­˜
                        cudaFree(d_A);
                        cudaFree(d_B);
                        cudaFree(d_C);
                        
                        // é‡Šæ”¾ä¸»æœºå†…å­˜
                        free(h_A);
                        free(h_B);
                        free(h_C);
                        
                        return 0;
                    }</code></pre>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>ä»£ç åˆ†æ</strong><br>
                            ä¸Šè¿°CUDAç¨‹åºå®ç°äº†ä¸¤ä¸ªå‘é‡çš„å¹¶è¡ŒåŠ æ³•ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>Code Analysis</strong><br>
                            The above CUDA program implements parallel addition of two vectors, mainly including the following parts:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ol class="zh-content">
                                <li><strong>æ ¸å‡½æ•°å®šä¹‰</strong>ï¼šä½¿ç”¨<code>__global__</code>å…³é”®å­—å®šä¹‰çš„<code>vectorAdd</code>å‡½æ•°ï¼Œå®ƒåœ¨GPUä¸Šå¹¶è¡Œæ‰§è¡Œ</li>
                                <li><strong>å†…å­˜ç®¡ç†</strong>ï¼šåˆ†é…ä¸»æœºå†…å­˜å’Œè®¾å¤‡å†…å­˜ï¼Œå¹¶åœ¨å®ƒä»¬ä¹‹é—´å¤åˆ¶æ•°æ®</li>
                                <li><strong>æ ¸å‡½æ•°å¯åŠ¨</strong>ï¼šä½¿ç”¨<code>&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;</code>è¯­æ³•å¯åŠ¨æ ¸å‡½æ•°</li>
                                <li><strong>ç»“æœéªŒè¯</strong>ï¼šå°†è®¡ç®—ç»“æœä»è®¾å¤‡å¤åˆ¶å›ä¸»æœºï¼Œå¹¶éªŒè¯å…¶æ­£ç¡®æ€§</li>
                                <li><strong>èµ„æºé‡Šæ”¾</strong>ï¼šé‡Šæ”¾åˆ†é…çš„ä¸»æœºå’Œè®¾å¤‡å†…å­˜</li>
                            </ol>
                            
                            <ol class="en-content" style="display: none;">
                                <li><strong>Kernel Definition</strong>: The <code>vectorAdd</code> function defined with the <code>__global__</code> keyword, which executes in parallel on the GPU</li>
                                <li><strong>Memory Management</strong>: Allocating host and device memory, and copying data between them</li>
                                <li><strong>Kernel Launch</strong>: Launching the kernel using the <code>&lt;&lt;&lt;blocksPerGrid, threadsPerBlock&gt;&gt;&gt;</code> syntax</li>
                                <li><strong>Result Verification</strong>: Copying the computation results from the device back to the host and verifying their correctness</li>
                                <li><strong>Resource Release</strong>: Releasing the allocated host and device memory</li>
                            </ol>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>æ ¸å‡½æ•°è¯¦è§£</strong><br>
                            æ ¸å‡½æ•°æ˜¯CUDAç¨‹åºçš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œå®ƒå®šä¹‰äº†åœ¨GPUä¸Šå¹¶è¡Œæ‰§è¡Œçš„ä»£ç ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œ<code>vectorAdd</code>æ ¸å‡½æ•°çš„å…³é”®ç‰¹ç‚¹åŒ…æ‹¬ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>Kernel Function Details</strong><br>
                            The kernel function is the core part of a CUDA program, defining the code to be executed in parallel on the GPU. In the example above, the key features of the <code>vectorAdd</code> kernel function include:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ul class="zh-content">
                                <li><strong>çº¿ç¨‹ç´¢å¼•è®¡ç®—</strong>ï¼š<code>int i = blockDim.x * blockIdx.x + threadIdx.x;</code>è®¡ç®—æ¯ä¸ªçº¿ç¨‹çš„å…¨å±€ç´¢å¼•</li>
                                <li><strong>è¾¹ç•Œæ£€æŸ¥</strong>ï¼š<code>if (i < numElements)</code>ç¡®ä¿çº¿ç¨‹ä¸ä¼šè®¿é—®æ•°ç»„è¾¹ç•Œä¹‹å¤–çš„å†…å­˜</li>
                                <li><strong>æ•°æ®å¹¶è¡Œæ“ä½œ</strong>ï¼šæ¯ä¸ªçº¿ç¨‹ç‹¬ç«‹æ‰§è¡Œç›¸åŒçš„æ“ä½œï¼ˆå‘é‡å…ƒç´ ç›¸åŠ ï¼‰</li>
                            </ul>
                            
                            <ul class="en-content" style="display: none;">
                                <li><strong>Thread Index Calculation</strong>: <code>int i = blockDim.x * blockIdx.x + threadIdx.x;</code> calculates the global index for each thread</li>
                                <li><strong>Boundary Check</strong>: <code>if (i < numElements)</code> ensures that threads do not access memory outside the array boundaries</li>
                                <li><strong>Data Parallel Operation</strong>: Each thread independently performs the same operation (adding vector elements)</li>
                            </ul>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>æ€»ç»“</strong><br>
                            CUDAç¼–ç¨‹åŸºç¡€åŒ…æ‹¬ç†è§£CUDAçš„ç¼–ç¨‹æ¨¡å‹ã€çº¿ç¨‹å±‚æ¬¡ç»“æ„å’Œå†…å­˜ç®¡ç†ã€‚é€šè¿‡ä½¿ç”¨CUDAæä¾›çš„APIå’Œè¯­è¨€æ‰©å±•ï¼Œå¼€å‘è€…å¯ä»¥ç¼–å†™é«˜æ•ˆçš„å¹¶è¡Œç¨‹åºï¼Œå……åˆ†åˆ©ç”¨GPUçš„å¼ºå¤§è®¡ç®—èƒ½åŠ›ã€‚ä¸Šè¿°å‘é‡åŠ æ³•ç¤ºä¾‹å±•ç¤ºäº†CUDAç¼–ç¨‹çš„åŸºæœ¬æµç¨‹å’Œæ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºè¿›ä¸€æ­¥å­¦ä¹ æ›´å¤æ‚çš„CUDAåº”ç”¨å¥ å®šäº†åŸºç¡€ã€‚<span class="emoji">ğŸ“</span></p>
                            
                            <p class="en-content" style="display: none;"><strong>Summary</strong><br>
                            CUDA programming basics include understanding the CUDA programming model, thread hierarchy, and memory management. By using the APIs and language extensions provided by CUDA, developers can write efficient parallel programs that fully utilize the powerful computing capabilities of GPUs. The vector addition example above demonstrates the basic process and core concepts of CUDA programming, laying the foundation for further learning of more complex CUDA applications.<span class="emoji">ğŸ“</span></p>
                        </div>
                    </section>
                    
                    <section id="section-4">
                        <h2 data-en="IV. Advanced CUDA Techniques" data-zh="å››ã€CUDAé«˜çº§æŠ€æœ¯">å››ã€CUDAé«˜çº§æŠ€æœ¯</h2>
                        
                        <h3 data-en="4.1 CUDA Memory Hierarchy" data-zh="4.1 CUDAå†…å­˜å±‚æ¬¡ç»“æ„">4.1 CUDAå†…å­˜å±‚æ¬¡ç»“æ„</h3>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>CUDAå†…å­˜ç±»å‹</strong><br>
                            CUDAæä¾›äº†å¤šç§ç±»å‹çš„å†…å­˜ï¼Œæ¯ç§å†…å­˜éƒ½æœ‰ä¸åŒçš„ä½œç”¨åŸŸã€ç”Ÿå‘½å‘¨æœŸå’Œæ€§èƒ½ç‰¹ç‚¹ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>CUDA Memory Types</strong><br>
                            CUDA provides multiple types of memory, each with different scope, lifetime, and performance characteristics:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ul class="zh-content">
                                <li><strong>å…¨å±€å†…å­˜ï¼ˆGlobal Memoryï¼‰</strong>ï¼šå®¹é‡æœ€å¤§ï¼Œä½†è®¿é—®å»¶è¿Ÿæœ€é«˜çš„å†…å­˜ï¼Œå¯è¢«æ‰€æœ‰çº¿ç¨‹è®¿é—®</li>
                                <li><strong>å…±äº«å†…å­˜ï¼ˆShared Memoryï¼‰</strong>ï¼šå—å†…å…±äº«çš„é«˜é€Ÿå†…å­˜ï¼Œå¯ç”¨äºçº¿ç¨‹é—´é€šä¿¡</li>
                                <li><strong>å¯„å­˜å™¨ï¼ˆRegistersï¼‰</strong>ï¼šæ¯ä¸ªçº¿ç¨‹ç§æœ‰çš„æœ€å¿«å†…å­˜ï¼Œç”¨äºå­˜å‚¨å±€éƒ¨å˜é‡</li>
                                <li><strong>å¸¸é‡å†…å­˜ï¼ˆConstant Memoryï¼‰</strong>ï¼šåªè¯»å†…å­˜ï¼Œé€‚åˆå­˜å‚¨ä¸å˜çš„æ•°æ®ï¼Œå…·æœ‰ç¼“å­˜åŠŸèƒ½</li>
                                <li><strong>çº¹ç†å†…å­˜ï¼ˆTexture Memoryï¼‰</strong>ï¼šé’ˆå¯¹2Dç©ºé—´å±€éƒ¨æ€§ä¼˜åŒ–çš„åªè¯»å†…å­˜</li>
                                <li><strong>æœ¬åœ°å†…å­˜ï¼ˆLocal Memoryï¼‰</strong>ï¼šå½“å¯„å­˜å™¨ä¸è¶³æ—¶ï¼Œç”¨äºå­˜å‚¨çº¿ç¨‹ç§æœ‰æ•°æ®çš„å†…å­˜</li>
                            </ul>
                            
                            <ul class="en-content" style="display: none;">
                                <li><strong>Global Memory</strong>: Largest capacity but highest access latency, accessible by all threads</li>
                                <li><strong>Shared Memory</strong>: High-speed memory shared within a block, can be used for inter-thread communication</li>
                                <li><strong>Registers</strong>: Fastest memory private to each thread, used to store local variables</li>
                                <li><strong>Constant Memory</strong>: Read-only memory suitable for storing unchanging data, with caching capability</li>
                                <li><strong>Texture Memory</strong>: Read-only memory optimized for 2D spatial locality</li>
                                <li><strong>Local Memory</strong>: Memory used to store thread-private data when registers are insufficient</li>
                            </ul>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>å†…å­˜ä¼˜åŒ–ç­–ç•¥</strong><br>
                            æœ‰æ•ˆåˆ©ç”¨CUDAå†…å­˜å±‚æ¬¡ç»“æ„æ˜¯ä¼˜åŒ–CUDAç¨‹åºæ€§èƒ½çš„å…³é”®ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>Memory Optimization Strategies</strong><br>
                            Effectively utilizing the CUDA memory hierarchy is key to optimizing CUDA program performance:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ul class="zh-content">
                                <li><strong>åˆå¹¶è®¿é—®ï¼ˆCoalesced Accessï¼‰</strong>ï¼šç¡®ä¿çº¿ç¨‹è®¿é—®è¿ç»­çš„å…¨å±€å†…å­˜åœ°å€ï¼Œä»¥æœ€å¤§åŒ–å†…å­˜å¸¦å®½</li>
                                <li><strong>åˆ©ç”¨å…±äº«å†…å­˜</strong>ï¼šå°†é¢‘ç¹è®¿é—®çš„æ•°æ®åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­ï¼Œå‡å°‘å…¨å±€å†…å­˜è®¿é—®</li>
                                <li><strong>é¿å…å¯„å­˜å™¨æº¢å‡º</strong>ï¼šæ§åˆ¶æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨çš„å¯„å­˜å™¨æ•°é‡ï¼Œé¿å…æº¢å‡ºåˆ°æœ¬åœ°å†…å­˜</li>
                                <li><strong>ä½¿ç”¨å¸¸é‡å†…å­˜</strong>ï¼šå°†åªè¯»æ•°æ®æ”¾åœ¨å¸¸é‡å†…å­˜ä¸­ï¼Œåˆ©ç”¨å…¶ç¼“å­˜æœºåˆ¶</li>
                                <li><strong>å†…å­˜é¢„å–</strong>ï¼šåœ¨æ•°æ®éœ€è¦ä¹‹å‰é¢„å…ˆåŠ è½½ï¼Œéšè—å†…å­˜è®¿é—®å»¶è¿Ÿ</li>
                            </ul>
                            
                            <ul class="en-content" style="display: none;">
                                <li><strong>Coalesced Access</strong>: Ensure threads access consecutive global memory addresses to maximize memory bandwidth</li>
                                <li><strong>Utilize Shared Memory</strong>: Load frequently accessed data into shared memory to reduce global memory access</li>
                                <li><strong>Avoid Register Spilling</strong>: Control the number of registers used by each thread to avoid spilling to local memory</li>
                                <li><strong>Use Constant Memory</strong>: Place read-only data in constant memory to leverage its caching mechanism</li>
                                <li><strong>Memory Prefetching</strong>: Load data before it is needed to hide memory access latency</li>
                            </ul>
                        </div>
                        
                        <h3 data-en="4.2 CUDA Optimization Techniques" data-zh="4.2 CUDAä¼˜åŒ–æŠ€æœ¯">4.2 CUDAä¼˜åŒ–æŠ€æœ¯</h3>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>çº¿ç¨‹é…ç½®ä¼˜åŒ–</strong><br>
                            åˆç†é…ç½®çº¿ç¨‹å—å¤§å°å’Œç½‘æ ¼å¤§å°å¯¹CUDAç¨‹åºæ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>Thread Configuration Optimization</strong><br>
                            Properly configuring block size and grid size has a significant impact on CUDA program performance:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ul class="zh-content">
                                <li><strong>çº¿ç¨‹å—å¤§å°</strong>ï¼šé€šå¸¸é€‰æ‹©32çš„å€æ•°ï¼ˆå¦‚128ã€256ï¼‰ï¼Œä»¥é€‚åº”CUDAçš„çº¿ç¨‹æŸï¼ˆwarpï¼‰å¤§å°</li>
                                <li><strong>å ç”¨ç‡ï¼ˆOccupancyï¼‰</strong>ï¼šå¹³è¡¡æ¯ä¸ªSMä¸Šæ´»è·ƒçš„çº¿ç¨‹å—æ•°é‡ï¼Œä»¥æœ€å¤§åŒ–å¹¶è¡Œåº¦</li>
                                <li><strong>èµ„æºä½¿ç”¨</strong>ï¼šè€ƒè™‘å¯„å­˜å™¨å’Œå…±äº«å†…å­˜ä½¿ç”¨é‡ï¼Œé¿å…èµ„æºé™åˆ¶å¯¼è‡´çš„ä½å ç”¨ç‡</li>
                            </ul>
                            
                            <ul class="en-content" style="display: none;">
                                <li><strong>Block Size</strong>: Usually choose multiples of 32 (such as 128, 256) to match the CUDA warp size</li>
                                <li><strong>Occupancy</strong>: Balance the number of active blocks per SM to maximize parallelism</li>
                                <li><strong>Resource Usage</strong>: Consider register and shared memory usage to avoid low occupancy due to resource limitations</li>
                            </ul>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>æŒ‡ä»¤çº§ä¼˜åŒ–</strong><br>
                            åœ¨æ ¸å‡½æ•°å±‚é¢çš„ä¼˜åŒ–æŠ€æœ¯ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>Instruction-Level Optimization</strong><br>
                            Optimization techniques at the kernel function level:</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <ul class="zh-content">
                                <li><strong>å¾ªç¯å±•å¼€</strong>ï¼šå‡å°‘å¾ªç¯æ§åˆ¶å¼€é”€ï¼Œå¢åŠ æŒ‡ä»¤çº§å¹¶è¡Œæ€§</li>
                                <li><strong>é¿å…åˆ†æ”¯å‘æ•£</strong>ï¼šåŒä¸€çº¿ç¨‹æŸå†…çš„çº¿ç¨‹åº”æ‰§è¡Œç›¸åŒçš„æ‰§è¡Œè·¯å¾„ï¼Œé¿å…æ¡ä»¶åˆ†æ”¯</li>
                                <li><strong>ä½¿ç”¨å¿«é€Ÿæ•°å­¦å‡½æ•°</strong>ï¼šä½¿ç”¨<code>__fdividef()</code>ç­‰å¿«é€Ÿä½†ç²¾åº¦ç•¥ä½çš„æ•°å­¦å‡½æ•°</li>
                                <li><strong>å‡å°‘åŒæ­¥ç‚¹</strong>ï¼šå°½é‡å‡å°‘<code>__syncthreads()</code>è°ƒç”¨ï¼Œé™ä½åŒæ­¥å¼€é”€</li>
                            </ul>
                            
                            <ul class="en-content" style="display: none;">
                                <li><strong>Loop Unrolling</strong>: Reduce loop control overhead and increase instruction-level parallelism</li>
                                <li><strong>Avoid Branch Divergence</strong>: Threads within the same warp should follow the same execution path, avoiding conditional branches</li>
                                <li><strong>Use Fast Math Functions</strong>: Use fast but slightly less precise math functions like <code>__fdividef()</code></li>
                                <li><strong>Reduce Synchronization Points</strong>: Minimize <code>__syncthreads()</code> calls to reduce synchronization overhead</li>
                            </ul>
                        </div>
                        
                        <h3 data-en="4.3 Advanced CUDA Features" data-zh="4.3 CUDAé«˜çº§ç‰¹æ€§">4.3 CUDAé«˜çº§ç‰¹æ€§</h3>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>åŠ¨æ€å¹¶è¡Œï¼ˆDynamic Parallelismï¼‰</strong><br>
                            CUDAåŠ¨æ€å¹¶è¡Œå…è®¸GPUçº¿ç¨‹ç›´æ¥å¯åŠ¨æ–°çš„æ ¸å‡½æ•°ï¼Œæ— éœ€CPUå‚ä¸ï¼Œé€‚ç”¨äºé€’å½’ç®—æ³•å’Œè‡ªé€‚åº”ç½‘æ ¼ç»†åŒ–ç­‰åœºæ™¯ã€‚</p>
                            
                            <p class="en-content" style="display: none;"><strong>Dynamic Parallelism</strong><br>
                            CUDA dynamic parallelism allows GPU threads to directly launch new kernel functions without CPU involvement, suitable for recursive algorithms and adaptive grid refinement scenarios.</p>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>ç»Ÿä¸€å†…å­˜ï¼ˆUnified Memoryï¼‰</strong><br>
                            CUDAç»Ÿä¸€å†…å­˜æä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„åœ°å€ç©ºé—´ï¼Œè‡ªåŠ¨ç®¡ç†ä¸»æœºå’Œè®¾å¤‡ä¹‹é—´çš„æ•°æ®ä¼ è¾“ï¼Œç®€åŒ–äº†å†…å­˜ç®¡ç†ï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>Unified Memory</strong><br>
                            CUDA Unified Memory provides a unified address space that automatically manages data transfer between host and device, simplifying memory management:</p>
                        </div>
                        
                        <div class="code-block">
                            <pre><code class="language-cpp">// ä½¿ç”¨ç»Ÿä¸€å†…å­˜çš„å‘é‡åŠ æ³•ç¤ºä¾‹
                    __global__ void vectorAdd(float *A, float *B, float *C, int numElements) {
                        int i = blockDim.x * blockIdx.x + threadIdx.x;
                        if (i < numElements) {
                            C[i] = A[i] + B[i];
                        }
                    }
                    
                    int main(void) {
                        int numElements = 50000;
                        size_t size = numElements * sizeof(float);
                        
                        // åˆ†é…ç»Ÿä¸€å†…å­˜
                        float *A, *B, *C;
                        cudaMallocManaged(&A, size);
                        cudaMallocManaged(&B, size);
                        cudaMallocManaged(&C, size);
                        
                        // åˆå§‹åŒ–æ•°æ®
                        for (int i = 0; i < numElements; ++i) {
                            A[i] = rand()/(float)RAND_MAX;
                            B[i] = rand()/(float)RAND_MAX;
                        }
                        
                        // å¯åŠ¨æ ¸å‡½æ•°
                        int threadsPerBlock = 256;
                        int blocksPerGrid = (numElements + threadsPerBlock - 1) / threadsPerBlock;
                        vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, numElements);
                        
                        // åŒæ­¥è®¾å¤‡
                        cudaDeviceSynchronize();
                        
                        // éªŒè¯ç»“æœï¼ˆç›´æ¥è®¿é—®ç»Ÿä¸€å†…å­˜ï¼‰
                        for (int i = 0; i < numElements; ++i) {
                            if (fabs(A[i] + B[i] - C[i]) > 1e-5) {
                                fprintf(stderr, "Result verification failed at element %d!\n", i);
                                exit(EXIT_FAILURE);
                            }
                        }
                        printf("Test PASSED\n");
                        
                        // é‡Šæ”¾ç»Ÿä¸€å†…å­˜
                        cudaFree(A);
                        cudaFree(B);
                        cudaFree(C);
                        
                        return 0;
                    }</code></pre>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>CUDAæµï¼ˆCUDA Streamsï¼‰</strong><br>
                            CUDAæµå…è®¸åœ¨GPUä¸Šå¹¶å‘æ‰§è¡Œå¤šä¸ªæ ¸å‡½æ•°å’Œå†…å­˜æ“ä½œï¼Œå®ç°ä»»åŠ¡çº§å¹¶è¡Œï¼š</p>
                            
                            <p class="en-content" style="display: none;"><strong>CUDA Streams</strong><br>
                            CUDA streams allow concurrent execution of multiple kernel functions and memory operations on the GPU, enabling task-level parallelism:</p>
                        </div>
                        
                        <div class="code-block">
                            <pre><code class="language-cpp">// ä½¿ç”¨CUDAæµçš„å¹¶å‘æ‰§è¡Œç¤ºä¾‹
                    int main(void) {
                        // åˆ›å»ºä¸¤ä¸ªCUDAæµ
                        cudaStream_t stream1, stream2;
                        cudaStreamCreate(&stream1);
                        cudaStreamCreate(&stream2);
                        
                        // åœ¨ä¸åŒæµä¸­æ‰§è¡Œæ ¸å‡½æ•°å’Œå†…å­˜æ“ä½œ
                        kernel1<<<blocks, threads, 0, stream1>>>(data1);
                        kernel2<<<blocks, threads, 0, stream2>>>(data2);
                        
                        cudaMemcpyAsync(h_data1, d_data1, size, cudaMemcpyDeviceToHost, stream1);
                        cudaMemcpyAsync(h_data2, d_data2, size, cudaMemcpyDeviceToHost, stream2);
                        
                        // åŒæ­¥æ‰€æœ‰æµ
                        cudaStreamSynchronize(stream1);
                        cudaStreamSynchronize(stream2);
                        
                        // é”€æ¯æµ
                        cudaStreamDestroy(stream1);
                        cudaStreamDestroy(stream2);
                        
                        return 0;
                    }</code></pre>
                        </div>
                        
                        <div class="bilingual-content">
                            <p class="zh-content"><strong>æ€»ç»“</strong><br>
                            CUDAé«˜çº§æŠ€æœ¯åŒ…æ‹¬æ·±å…¥ç†è§£å†…å­˜å±‚æ¬¡ç»“æ„ã€ä¼˜åŒ–çº¿ç¨‹é…ç½®å’ŒæŒ‡ä»¤æ‰§è¡Œï¼Œä»¥åŠåˆ©ç”¨åŠ¨æ€å¹¶è¡Œã€ç»Ÿä¸€å†…å­˜å’ŒCUDAæµç­‰é«˜çº§ç‰¹æ€§ã€‚æŒæ¡è¿™äº›æŠ€æœ¯å¯ä»¥æ˜¾è‘—æé«˜CUDAç¨‹åºçš„æ€§èƒ½ï¼Œå……åˆ†å‘æŒ¥GPUçš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”æ ¹æ®å…·ä½“é—®é¢˜å’Œç¡¬ä»¶ç‰¹æ€§ï¼Œé€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥å’Œé«˜çº§ç‰¹æ€§ï¼Œä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚<span class="emoji">ğŸ“</span></p>
                            
                            <p class="en-content" style="display: none;"><strong>Summary</strong><br>
                            Advanced CUDA techniques include a deep understanding of the memory hierarchy, optimizing thread configuration and instruction execution, and utilizing advanced features such as dynamic parallelism, unified memory, and CUDA streams. Mastering these techniques can significantly improve the performance of CUDA programs, fully leveraging the parallel computing capabilities of GPUs. In practical applications, appropriate optimization strategies and advanced features should be chosen based on the specific problem and hardware characteristics to achieve optimal performance.<span class="emoji">ğŸ“</span></p>
                        </div>
                    </section>
                    
                    <!-- ç»“è¯­ -->
                    <div class="post-conclusion">
                        <h2 data-en="Conclusion" data-zh="ç»“è¯­">ç»“è¯­</h2>
                        
                        <div class="bilingual-content">
                            <p class="zh-content">æ€»ç»“æ–‡ç« çš„ä¸»è¦è§‚ç‚¹å’Œç»“è®ºã€‚æä¾›è¿›ä¸€æ­¥æ€è€ƒæˆ–åç»­å­¦ä¹ çš„å»ºè®®ã€‚</p>
                            
                            <p class="en-content" style="display: none;">Summarize the main points and conclusions of the article. Provide suggestions for further thinking or subsequent learning.</p>
                            
                            <p class="zh-content">æ„Ÿè°¢é˜…è¯»ï¼å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨è¯„è®ºåŒºç•™è¨€ã€‚ <span class="emoji">ğŸ’»âœ¨</span></p>
                            
                            <p class="en-content" style="display: none;">Thanks for reading! If you have any questions or suggestions, please leave a message in the comments section. <span class="emoji">ğŸ’»âœ¨</span></p>
                        </div>
                        
                        <div class="post-signature">
                            <p>â€” HealthJian <span class="emoji">âœï¸</span></p>
                        </div>
                    </div>
                </div>
            </article>
            
            <!-- è¯„è®ºåŒº -->
            <div class="comments-section">
                <h3 data-en="Comments" data-zh="è¯„è®º">è¯„è®º</h3>
                <div class="comment-form">
                    <textarea placeholder="å†™ä¸‹ä½ çš„æƒ³æ³•..." data-en-placeholder="Write your thoughts..." data-zh-placeholder="å†™ä¸‹ä½ çš„æƒ³æ³•..."></textarea>
                    <button data-en="Submit" data-zh="æäº¤">æäº¤</button>
                </div>
                <div class="comments-container">
                    <p class="no-comments" data-en="Be the first to comment!" data-zh="æˆä¸ºç¬¬ä¸€ä¸ªè¯„è®ºçš„äººï¼">æˆä¸ºç¬¬ä¸€ä¸ªè¯„è®ºçš„äººï¼ <span class="emoji">ğŸ‰</span></p>
                </div>
            </div>
        </main>
    </div>

    <!-- é¡µè„š -->
    <footer>
        <div class="footer-content">
            <div class="footer-info">
                <p>&copy; 2025 CaiNiaojian&HealthJian all followed.</p>
                <p data-en="Contact: " data-zh="è”ç³»æ–¹å¼ï¼š">è”ç³»æ–¹å¼ï¼š<a href="mailto:gaojian1573@foxmail.com">gaojian1573@foxmail.com</a></p>
                <p data-en="Location: " data-zh="åœ°å€ï¼š">åœ°å€ï¼šXX</p>
            </div>
            <div class="footer-links">
                <a href="../../../blog.html" data-en="Blog" data-zh="åšå®¢">åšå®¢</a>
                <a href="../../../about.html" data-en="About" data-zh="å…³äº">å…³äº</a>
                <a href="https://github.com/CaiNiaojian" target="_blank" data-en="GitHub" data-zh="GitHub">GitHub</a>
                <a href="../../../changelog.html" data-en="ChangeLog" data-zh="æ›´æ–°æ—¥å¿—">æ›´æ–°æ—¥å¿—</a>
            </div>
        </div>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // åˆå§‹åŒ–ä»£ç é«˜äº®
            hljs.highlightAll();
        });
    </script>
    <script src="../../../js/main.js"></script>
    <script src="../../../js/theme.js"></script>
    <script src="../../../js/language.js"></script>
    <script src="../../../js/blog-post.js"></script>
</body>
</html> 