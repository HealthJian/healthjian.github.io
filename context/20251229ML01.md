# 机器学习复习回忆资料（第一篇：基础核心概念）

## 一、机器学习的定义与本质

1.  核心定义：机器学习是人工智能的核心分支，研究如何让计算机通过**学习数据**而非显式编程，获得解决问题的能力。其核心目标是从数据中挖掘规律（构建模型），并利用该规律对未知数据进行预测或决策。

2.  本质理解：机器学习的本质是“从经验中学习”——这里的“经验”即数据，“学习”即通过算法优化模型参数，使模型在特定任务上的性能逐步提升。简单来说，就是让计算机“自学成才”，而非依赖人类预先编写的固定规则。

3.  与传统编程的区别：传统编程是“输入规则+数据 → 输出结果”；机器学习是“输入数据+结果 → 输出规则”（即模型），再用规则处理新数据。

## 二、机器学习的核心要素

机器学习系统的构建离不开以下4个核心要素，缺一不可：

1. **数据（Data）**：模型学习的基础，是“经验”的载体。分为训练数据（用于训练模型）、验证数据（用于调优参数）、测试数据（用于评估模型泛化能力）。数据的质量（准确性、完整性）和数量直接决定模型上限（“数据决定上限，算法逼近上限”）。

2. **模型（Model）**：用于描述数据规律的数学表达式或计算框架（如线性方程、决策树、神经网络等）。模型本质是“假设空间”的一个子集，即我们认为数据可能遵循的规律范围。

3. **损失函数（Loss Function）**：衡量模型预测结果与真实结果差距的指标，是模型优化的“指南针”。损失函数值越小，说明模型预测越准确。常见类型：回归任务用均方误差（MSE）、平均绝对误差（MAE）；分类任务用交叉熵（Cross-Entropy）、0-1损失。

4. **优化算法（Optimization Algorithm）**：用于最小化损失函数的方法，核心是调整模型参数以找到最优解。常见类型：梯度下降法（GD）及其变体（随机梯度下降SGD、批量梯度下降BGD、小批量梯度下降MBGD）、牛顿法、拟牛顿法等。其中梯度下降是最基础且应用最广泛的优化算法。

## 三、机器学习的主要分类（按学习方式）

根据训练数据是否包含“标签”（真实结果），以及学习方式的不同，机器学习主要分为三大类：

### 1.  监督学习（Supervised Learning）

· 核心特点：训练数据包含**输入特征**和对应的**标签**（即“有老师指导”的学习），模型通过学习特征与标签的映射关系，实现对新数据的标签预测。

· 核心任务：分为回归（标签是连续值）和分类（标签是离散值）。

· 常见算法：

- 回归任务：线性回归（单变量/多变量）、多项式回归、岭回归、Lasso回归、支持向量回归（SVR）、决策树回归、随机森林回归等。

- 分类任务：逻辑回归（二分类）、K近邻（KNN）、决策树、随机森林、梯度提升树（GBDT）、XGBoost、LightGBM、支持向量机（SVM）、朴素贝叶斯等。

· 典型应用：房价预测（回归）、垃圾邮件识别（分类）、图像识别（分类）、疾病诊断（分类）等。

### 2.  无监督学习（Unsupervised Learning）

· 核心特点：训练数据**只有输入特征，没有标签**（即“无老师指导”的学习），模型通过挖掘数据本身的内在结构（如聚类、分布）来学习规律。

· 核心任务：聚类（将相似数据归为一类）、降维（减少特征维度，保留关键信息）、密度估计（估计数据的概率分布）。

· 常见算法：

- 聚类：K均值聚类（K-Means）、层次聚类（Hierarchical Clustering）、DBSCAN（密度聚类）、谱聚类等。

- 降维：主成分分析（PCA）、线性判别分析（LDA，注：LDA可监督可无监督，核心用于降维）、t-SNE（非线性降维，常用于可视化）、自动编码器（AE，深度学习类降维方法）等。

· 典型应用：用户分群（聚类）、图像压缩（降维）、异常检测（密度估计）、推荐系统中的用户兴趣挖掘（聚类）等。

### 3.  强化学习（Reinforcement Learning）

· 核心特点：通过“智能体（Agent）与环境（Environment）的交互”学习——智能体执行动作后，环境会反馈“奖励”或“惩罚”，智能体的目标是通过不断试错，找到能最大化累积奖励的动作策略。

· 核心要素：智能体（Agent）、环境（Environment）、状态（State）、动作（Action）、奖励（Reward）、策略（Policy）。

· 常见算法：Q-Learning、SARSA、深度Q网络（DQN）、策略梯度（Policy Gradient）、Actor-Critic等。

· 典型应用：游戏AI（如AlphaGo）、机器人导航、自动驾驶、资源调度等。

## 四、其他重要分类补充

1. **半监督学习（Semi-Supervised Learning）**：介于监督学习和无监督学习之间，训练数据部分有标签、部分无标签。适用于标签获取成本高的场景（如医疗数据），常见算法：自训练、协同训练、生成式模型等。

2. **弱监督学习（Weakly Supervised Learning）**：训练数据的标签不精确（如模糊标签、不完整标签、噪声标签），模型需要从“弱标签”中学习有效规律，常见于图像识别、自然语言处理等领域。

3. **在线学习（Online Learning）**：数据是流式输入的，模型边接收数据边学习（实时更新参数），适用于数据量大、实时性要求高的场景（如推荐系统、金融交易），核心优势是占用内存小、响应快。

4. **批量学习（Batch Learning）**：一次性使用所有训练数据训练模型，训练完成后参数固定，若要适应新数据需重新训练。适用于数据量固定、实时性要求低的场景，优势是模型训练充分、稳定性强。

## 五、核心考点与易错点提示


      1.  监督/无监督/强化学习的核心区别：关键看训练数据是否有标签、学习方式是“学习映射关系”还是“挖掘结构”还是“交互试错”。
      2.  损失函数与任务匹配：回归任务用MSE/MAE，分类任务用交叉熵，不可混用（如用MSE做分类会导致梯度消失）。
      3.  优化算法的核心逻辑：梯度下降的“梯度”是损失函数对参数的偏导数，方向是损失函数增大的方向，因此需要“反向更新参数”（减去学习率×梯度）。
      4.  数据划分的意义：测试数据必须与训练数据独立同分布（i.i.d.），否则模型评估结果无效；验证数据的作用是调优超参数（如KNN的K值、正则化系数），不能用于训练参数。
    
