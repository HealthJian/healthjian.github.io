# 机器学习复习回忆资料（第四篇：无监督学习核心算法——K-Means与PCA）

说明：前三篇聚焦监督学习，需依赖带标签的数据训练模型。而无监督学习无需标签，核心目标是挖掘数据本身的内在结构（如聚类）或简化数据维度（如降维），广泛应用于数据探索、特征工程、异常检测等场景。K-Means是最经典的聚类算法，PCA是最常用的降维算法，二者是无监督学习的基础，需重点掌握其核心逻辑、实现步骤及适用边界。

## 一、K-Means聚类（K-Means Clustering）

### 1.  算法定位与核心思想

K-Means是一种**划分式聚类算法**，核心目标是将无标签的数据集划分为K个不重叠的子集（称为“簇”），使得每个簇内的样本尽可能相似（簇内紧凑），不同簇的样本尽可能不同（簇间分离）。

核心思想：通过迭代优化找到K个“簇中心”（质心），每个样本被分配到距离最近的簇中心所在的簇，最终使所有样本到其所属簇中心的距离之和最小化。

核心特点：原理简单、计算效率高、易于实现，适用于大规模数据集；但需预先指定簇数K，且对初始簇中心和异常值敏感。

### 2.  核心原理与实现步骤

#### （1）核心距离度量

K-Means使用“欧氏距离”衡量样本与簇中心的相似度（距离越小，相似度越高），欧氏距离公式：

对于样本 \( x_i = (x_{i1}, x_{i2}, ..., x_{in}) \) 和簇中心 \( \mu_j = (\mu_{j1}, \mu_{j2}, ..., \mu_{jn}) \)，距离 \( d(x_i, \mu_j) = \sqrt{\sum_{k=1}^n (x_{ik} - \mu_{jk})^2} \)。

说明：使用欧氏距离前需对数据做归一化/标准化（如Min-Max缩放、Z-Score标准化），否则数值范围大的特征会主导距离计算，影响聚类结果。

#### （2）迭代优化步骤（Lloyd算法）

K-Means通过“初始化→分配→更新→收敛”四步迭代实现优化，具体步骤：

1. **初始化簇中心**：从数据集中随机选择K个样本作为初始簇中心 \( \mu_1, \mu_2, ..., \mu_K \)（关键步骤，初始值会影响最终结果）。

2. **样本分配**：计算每个样本到K个簇中心的欧氏距离，将样本分配到距离最近的簇中心所在的簇（即给每个样本打上簇标签）。

3. **更新簇中心**：对每个簇，计算簇内所有样本的特征均值，将该均值作为新的簇中心（新簇中心 = 簇内样本特征均值向量）。

4. **判断收敛**：若所有簇中心的位置在迭代前后变化小于预设阈值（如0.001），或迭代次数达到最大值，则停止迭代；否则返回步骤2继续迭代。

#### （3）目标函数（损失函数）

K-Means的目标是最小化“簇内平方和（Within-Cluster Sum of Squares, WCSS）”，也称为“惯性（Inertia）”，目标函数公式：

\( J = \sum_{j=1}^K \sum_{x_i \in C_j} d(x_i, \mu_j)^2 \)

说明：\( C_j \) 是第j个簇的样本集合，\( d(x_i, \mu_j)^2 \) 是样本 \( x_i \) 到簇中心 \( \mu_j \) 的欧氏距离平方。WCSS越小，说明簇内样本越紧凑。

### 3.  关键问题与解决方案

#### （1）如何确定最优簇数K？

K是K-Means的核心超参数，需手动指定，常用“肘部法则（Elbow Method）”选择最优K：

1. 遍历多个可能的K值（如K=1到10），对每个K训练K-Means模型，计算对应的WCSS。

2. 以K为横轴、WCSS为纵轴绘制折线图，寻找折线“肘部”对应的K值（即WCSS下降速度突然变缓的点）。

补充方法：① 轮廓系数（Silhouette Coefficient）：衡量样本聚类的合理性，轮廓系数越接近1，聚类效果越好；② 结合业务场景（如用户分群需结合实际运营需求确定K）。

#### （2）初始簇中心的影响与优化

随机初始化簇中心可能导致模型收敛到局部最优解（不同初始值得到不同聚类结果），解决方案：

1. **K-Means++算法**：优化初始簇中心的选择，使初始簇中心尽可能远离：① 随机选择1个样本作为第一个簇中心；② 计算其他样本到已选簇中心的距离，距离越大的样本被选中作为下一个簇中心的概率越高；③ 重复直至选够K个簇中心。

2. 多次运行：用不同初始值多次训练模型，选择WCSS最小的结果作为最终聚类结果。

#### （3）异常值的影响与处理

异常值会显著拉高WCSS，导致簇中心偏移，影响聚类效果。处理方法：① 聚类前通过箱线图、3σ原则等识别并移除异常值；② 采用对异常值更鲁棒的聚类算法（如K-Medoids，用簇内样本的中位数作为簇中心，而非均值）。

### 4.  优缺点与适用场景

#### （1）优点

- 原理简单直观，易于理解和实现；

- 计算复杂度低（时间复杂度 \( O(tKn) \)，t为迭代次数，K为簇数，n为样本数），适用于大规模数据集；

- 聚类结果可解释性较强（簇中心可理解为该簇的“代表样本”）。

#### （2）缺点

- 需预先指定簇数K，K的选择无统一标准，依赖经验和业务场景；

- 对初始簇中心敏感，易收敛到局部最优；

- 对异常值敏感，仅适用于凸形簇（球形、椭圆形等），不适用于非凸形簇（如环形、螺旋形数据）；

- 假设簇内样本服从正态分布，对非正态分布数据聚类效果较差。

#### （3）适用场景

用户分群（如电商用户消费行为分群）、图像分割、文档聚类（文本主题划分）、异常检测（聚类后远离所有簇中心的样本视为异常）、数据预处理（聚类后用簇标签作为新特征，辅助监督学习）。

### 5.  核心考点与易错点


      1.  K-Means的核心假设：簇是凸形的，簇内样本服从正态分布；若数据是非凸形簇，聚类效果差。
      2.  数据预处理的必要性：使用欧氏距离前必须做归一化/标准化，否则数值范围大的特征会主导距离计算。
      3.  最优K的选择：肘部法则是常用方法，但需结合业务场景；轮廓系数可辅助验证聚类效果（越接近1越好）。
      4.  K-Means与K-Medoids的区别：K-Means用均值做簇中心，对异常值敏感；K-Medoids用中位数做簇中心，对异常值鲁棒，但计算复杂度更高。
      5.  迭代收敛的条件：簇中心位置变化小于阈值，或迭代次数达到最大值，二者满足其一即可停止。
    

## 二、主成分分析（PCA，Principal Component Analysis）

### 1.  算法定位与核心思想

PCA是一种**线性降维算法**，核心目标是在“尽可能保留数据原始信息”的前提下，将高维特征空间映射到低维特征空间（即减少特征维度），去除冗余特征，简化模型计算，同时避免过拟合。

核心思想：找到一组正交的“主成分”（新的特征向量），使得数据在这些主成分上的投影方差最大（方差越大，保留的信息越多）。第一个主成分保留数据最多信息，第二个主成分与第一个主成分正交且保留剩余信息中最多的部分，以此类推。

核心特点：无监督降维，不依赖标签；降维后的数据是原始数据的线性组合；可去除特征间的相关性（冗余）。

### 2.  核心原理与实现步骤

#### （1）核心概念：方差与协方差矩阵

· 方差：衡量单个特征的离散程度，方差越大，特征包含的信息越多。

· 协方差：衡量两个特征之间的线性相关程度，协方差为0表示两个特征线性无关；协方差为正表示正相关，为负表示负相关。

· 协方差矩阵：将所有特征的方差和协方差组织成矩阵，对角线元素是各特征的方差，非对角线元素是两两特征的协方差。PCA的核心是对协方差矩阵进行特征分解。

#### （2）完整实现步骤

1. **数据标准化**：对原始数据的每个特征做Z-Score标准化（均值为0，方差为1）。原因：若不同特征的数值范围差异大，方差大的特征会主导主成分方向，影响降维结果。

2. **计算协方差矩阵**：对标准化后的数据，计算特征间的协方差矩阵 \( \Sigma \)（维度为 \( n \times n \)，n为原始特征数）。

3. **特征分解**：对协方差矩阵 \( \Sigma \) 进行特征分解，得到特征值（\( \lambda_1 \geq \lambda_2 \geq ... \geq \lambda_n \)）和对应的特征向量（\( \boldsymbol{u}_1, \boldsymbol{u}_2, ..., \boldsymbol{u}_n \)）。

4. **选择主成分**：根据特征值的大小选择前k个特征向量作为主成分（特征值越大，对应的主成分保留的信息越多）。k是降维后的维度，需手动指定或通过累计方差贡献率确定。

5. **数据投影**：将标准化后的原始数据矩阵与前k个特征向量组成的矩阵相乘，得到降维后的k维数据。

#### （3）关键指标：累计方差贡献率

累计方差贡献率用于确定最优的降维维度k，公式：

累计方差贡献率 \( = \frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^n \lambda_i} \)

说明：通常选择累计方差贡献率达到80%~90%的最小k值作为降维后的维度，确保保留大部分原始信息。例如，若前3个特征值的累计方差贡献率达到85%，则可将原始n维数据降为3维。

### 3.  关键问题与注意事项

#### （1）数据标准化的必要性

PCA对特征的数值范围敏感，若不标准化，数值范围大的特征（如“收入：10000~100000元”）会有更大的方差，其对应的特征向量会被选为主要主成分，导致降维结果偏向该特征，忽略其他数值范围小的特征（如“年龄：20~60岁”）。因此，标准化是PCA的必要前置步骤。

#### （2）主成分的可解释性

降维后的主成分是原始特征的线性组合，其物理意义不如原始特征直观。例如，“主成分1 = 0.6×收入 + 0.4×消费金额”，需结合业务场景解释主成分的含义（如该主成分可代表“消费能力”）。

#### （3）PCA与特征选择的区别

很多人会混淆PCA与特征选择，二者核心区别：

- PCA：通过线性组合生成新的低维特征（主成分），是“特征变换”，不是保留原始特征；

- 特征选择：从原始特征中直接筛选出部分重要特征，保留原始特征的物理意义，不生成新特征。

#### （4）适用数据类型

PCA是线性降维算法，仅适用于数据呈线性分布的场景；若数据存在非线性关系（如螺旋形数据），需使用非线性降维算法（如t-SNE、LLE、自动编码器AE等）。

### 4.  优缺点与适用场景

#### （1）优点

- 能有效降低特征维度，去除冗余特征，简化模型计算，提升训练效率；

- 去除特征间的相关性，避免多重共线性（对线性回归、逻辑回归等模型友好）；

- 原理清晰，实现简单，广泛适用于各种高维数据场景。

#### （2）缺点

- 线性降维，不适用于非线性分布数据；

- 主成分的可解释性较差，不如原始特征直观；

- 对异常值敏感，异常值会影响协方差矩阵的计算，导致主成分方向偏移；

- 降维过程会损失部分信息，若k选择不当，可能导致模型性能下降。

#### （3）适用场景

高维数据降维（如图像像素数据、文本TF-IDF特征）、特征工程（去除冗余特征，提升监督学习模型性能）、数据可视化（将高维数据降为2维/3维，便于直观观察数据分布）、噪声去除（低方差的主成分可能对应噪声，舍弃后可实现去噪）。

### 5.  核心考点与易错点


      1.  PCA的前置步骤：必须对数据做标准化（Z-Score），否则会受特征数值范围影响，导致降维结果偏差。
      2.  主成分的选择依据：特征值大小（越大保留信息越多）和累计方差贡献率（通常80%~90%）。
      3.  PCA的本质：是线性降维，通过对协方差矩阵特征分解找到主成分，实现数据的低维投影，核心是保留数据的最大方差。
      4.  与非线性降维的区别：PCA适用于线性分布数据，t-SNE、LLE适用于非线性分布数据（常用于高维数据可视化）。
      5.  异常值的影响：异常值会扭曲协方差矩阵，需在PCA前通过箱线图、3σ原则等移除异常值。

## 三、K-Means与PCA的核心区别与联系

|对比维度|K-Means|PCA|
|---|---|---|
|核心目标|将数据划分为K个簇，实现簇内紧凑、簇间分离|降低特征维度，保留数据核心信息|
|算法类型|聚类算法（无监督，无标签）|降维算法（无监督，无标签）|
|核心逻辑|迭代优化簇中心，最小化簇内平方和|对协方差矩阵特征分解，选择方差最大的主成分|
|数据预处理|需归一化/标准化（欧氏距离敏感）|需标准化（特征数值范围敏感）|
|输出结果|每个样本的簇标签（离散值）|降维后的低维特征（连续值）|
|内在联系|可联合使用：先用PCA对高维数据降维，再用K-Means对降维后的数据聚类（减少计算量，去除噪声干扰，提升聚类效果）||
