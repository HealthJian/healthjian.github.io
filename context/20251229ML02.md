# 机器学习复习回忆资料（第二篇：监督学习核心算法——线性回归与逻辑回归）

说明：监督学习是机器学习中应用最广泛的领域，而线性回归（回归任务基础）和逻辑回归（分类任务基础）是监督学习的入门核心算法，二者均基于线性模型框架，是理解后续复杂算法（如梯度提升树、神经网络）的基础，需重点掌握。

## 一、线性回归（Linear Regression）

### 1.  算法定位与核心目标

线性回归是**回归任务**的基础算法，核心思想是假设输入特征与输出标签之间存在**线性相关关系**，通过学习找到最优的线性模型参数，使得模型对标签的预测值与真实值的误差最小化。

核心目标：构建线性模型 \( y = f(x) \)，其中 \( x \) 是输入特征，\( y \) 是连续型输出标签，最小化预测值与真实值的差距。

### 2.  模型表达式

1. **单变量线性回归**（仅1个输入特征）：

2. **多变量线性回归**（多个输入特征）：

### 3.  损失函数与优化方法

#### （1）损失函数：均方误差（MSE）

线性回归的核心损失函数是均方误差，定义为预测值与真实值差值的平方和的平均值：

公式：\( L(w, b) = \frac{1}{m} \sum_{i=1}^m (\hat{y}_i - y_i)^2 = \frac{1}{m} \sum_{i=1}^m (w^T x_i + b - y_i)^2 \)

说明：\( m \) 是训练样本数量；\( \hat{y}_i \) 是第 \( i \) 个样本的预测值；\( y_i \) 是第 \( i \) 个样本的真实标签。MSE 是凸函数，存在唯一全局最小值，适合用梯度下降法优化。

#### （2）优化方法

1. **梯度下降法（Gradient Descent, GD）**：

2. **正规方程法（Normal Equation）**：

### 4.  正则化：解决过拟合问题

#### （1）过拟合现象

线性回归若特征过多或模型复杂，可能出现“过拟合”——模型在训练数据上误差极小，但在测试数据上误差极大，泛化能力差。本质是模型学习了训练数据中的噪声，而非通用规律。

#### （2）正则化方法

1. **岭回归（Ridge Regression）**：

2. **Lasso回归（Lasso Regression）**：

### 5.  核心考点与易错点


      1.  线性回归的前提假设：输入特征与标签线性相关、误差项独立同分布（i.i.d.）且服从正态分布、无多重共线性（特征间相关性过高会导致参数估计不稳定）。
      2.  梯度下降与正规方程的适用场景：高维数据用梯度下降，低维数据用正规方程。
      3.  正则化的作用：仅惩罚权重，不惩罚偏置项（避免模型欠拟合）；L1正则化可特征选择，L2正则化仅平滑权重。
      4.  过拟合的解决办法：正则化、减少特征维度、增加训练数据、早停（梯度下降中损失函数不再下降时停止迭代）。
    

## 二、逻辑回归（Logistic Regression）

### 1.  算法定位与核心目标

逻辑回归虽名为“回归”，实则是**二分类任务**的基础算法。核心思想是通过“Sigmoid函数”将线性模型的输出（连续值）映射到[0,1]区间，得到样本属于正类的概率，再根据概率判断类别。

核心目标：学习线性模型参数，使模型对正负类样本的概率预测尽可能接近真实标签（0或1）。

### 2.  模型表达式

1. **线性得分层**：先计算输入特征的线性组合（与线性回归一致）：\( z = w^T x + b \)

2. **Sigmoid激活层**：将线性得分 \( z \) 映射为概率 \( \hat{p} \)（样本属于正类的概率）：

### 3.  损失函数与优化方法

#### （1）损失函数：交叉熵损失（Cross-Entropy Loss）

为何不用MSE？：Sigmoid函数+MSE会导致损失函数是非凸函数（存在多个局部最小值），梯度下降易陷入局部最优；交叉熵损失是凸函数，优化更稳定，且梯度更新速度与误差大小相关（误差大时更新快，误差小时更新慢）。

交叉熵损失函数（单个样本）：\( L(w, b) = -[y \log \hat{p} + (1 - y) \log (1 - \hat{p})] \)

批量损失函数（\( m \) 个样本）：\( L(w, b) = -\frac{1}{m} \sum_{i=1}^m [y_i \log \hat{p}_i + (1 - y_i) \log (1 - \hat{p}_i)] \)

说明：\( y_i \) 是第 \( i \) 个样本的真实标签（0或1）；\( \hat{p}_i \) 是预测概率。当 \( y_i=1 \) 时，损失为 \( -\log \hat{p}_i \)（\( \hat{p}_i \) 越接近1，损失越小）；当 \( y_i=0 \) 时，损失为 \( -\log (1 - \hat{p}_i) \)（\( \hat{p}_i \) 越接近0，损失越小）。

#### （2）优化方法

逻辑回归的损失函数无解析解，需用梯度下降法迭代优化，核心是计算损失函数对参数 \( w \) 和 \( b \) 的梯度：

梯度公式：

\( \frac{\partial L}{\partial w} = \frac{1}{m} \sum_{i=1}^m (\hat{p}_i - y_i) x_i \)

\( \frac{\partial L}{\partial b} = \frac{1}{m} \sum_{i=1}^m (\hat{p}_i - y_i) \)

参数更新公式与线性回归一致（\( w = w - \eta \cdot \frac{\partial L}{\partial w} \)，\( b = b - \eta \cdot \frac{\partial L}{\partial b} \)），常用小批量梯度下降法。

### 4.  多分类逻辑回归（Softmax Regression）

逻辑回归可扩展为多分类任务（Softmax回归），核心是用Softmax函数替代Sigmoid函数，将线性得分映射为多个类别的概率（概率和为1）。

模型表达式：

线性得分：\( z_k = w_k^T x + b_k \)（\( k=1,2,...,K \)，\( K \) 是类别数）

Softmax函数：\( \hat{p}_k = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}} \)（第 \( k \) 类的概率）

损失函数：多分类交叉熵损失 \( L = -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_{ik} \log \hat{p}_{ik} \)（\( y_{ik} \) 是第 \( i \) 个样本属于第 \( k \) 类的指示变量，1表示是，0表示否）。

### 5.  正则化与过拟合解决

逻辑回归同样可能过拟合（如特征过多、参数过大），解决方案与线性回归类似，在交叉熵损失函数中加入L1或L2正则项：

· L2正则化（常用）：\( L = -\frac{1}{m} \sum_{i=1}^m [y_i \log \hat{p}_i + (1 - y_i) \log (1 - \hat{p}_i)] + \frac{\lambda}{2} \sum_{j=1}^n w_j^2 \)

· L1正则化：\( L = -\frac{1}{m} \sum_{i=1}^m [y_i \log \hat{p}_i + (1 - y_i) \log (1 - \hat{p}_i)] + \lambda \sum_{j=1}^n |w_j| \)（可实现特征选择）

### 6.  核心考点与易错点


      1.  逻辑回归的核心本质：是“线性模型+激活函数”的二分类模型，决策边界是线性的（适用于线性可分数据）。      2.  损失函数的选择：必须用交叉熵损失，不能用MSE，否则优化困难。
      3.  Sigmoid函数的特性：值域[0,1]，导数 \( \sigma'(z) = \sigma(z)(1 - \sigma(z)) \)，在 \( z=0 \) 处导数最大（0.25），在 \( z \to \pm\infty \) 处导数趋近于0（梯度消失）。
      4.  多分类扩展：Softmax回归是逻辑回归的多分类版本，决策边界仍是线性的；若数据线性不可分，需先对特征进行非线性变换（如核方法）。
      5.  适用场景：适合处理高维数据（如文本分类），模型简单、可解释性强、训练速度快。
    

## 三、线性回归与逻辑回归的核心区别

|对比维度|线性回归|逻辑回归|
|---|---|---|
|任务类型|回归任务（输出连续值）|分类任务（输出概率/离散标签）|
|模型输出|线性组合结果（\( w^T x + b \)）|Sigmoid/Softmax映射后的概率|
|损失函数|均方误差（MSE）|交叉熵损失|
|优化特性|损失函数是凸函数，可通过正规方程或梯度下降求解|损失函数是凸函数，仅能通过梯度下降求解（无解析解）|
|核心假设|特征与标签线性相关，误差项正态分布|特征与标签的线性组合可通过Sigmoid映射为概率|
