# 机器学习复习回忆资料（第三篇：监督学习进阶算法——决策树与集成学习）

说明：前两篇介绍的线性回归、逻辑回归均为线性模型，适用于数据线性可分的场景。但现实中多数数据存在非线性关系，决策树作为经典的非线性模型，可通过“分而治之”拟合复杂数据；而集成学习则通过组合多个基础模型（如决策树），进一步提升模型的泛化能力，是工业界常用的高性能算法，需重点掌握其核心逻辑与应用场景。

## 一、决策树（Decision Tree）

### 1.  算法定位与核心思想

决策树是一种**树形结构**的监督学习算法，可同时处理分类和回归任务（分别称为分类树、回归树）。核心思想是“分而治之”：从根节点开始，通过对输入特征的逐步判断（分支），将复杂数据集划分为多个子集，最终在叶节点输出类别（分类）或数值（回归）。

核心优势：模型结构直观、可解释性极强（类似人类的决策逻辑），无需对数据做预处理（如归一化、标准化），能自动捕捉特征间的交互关系。

### 2.  基本结构

决策树由三类节点组成，自上而下依次为：

1. **根节点**：整个决策树的起点，包含全部训练样本。

2. **内部节点**：对应特征的判断条件（如“年龄>30？”“收入≤50k？”），每个内部节点将样本划分为多个子集（分支）。

3. **叶节点**：决策树的终点，每个叶节点对应一个预测结果（分类任务为类别，回归任务为数值），叶节点中的样本属于同一类或数值相近。

### 3.  核心问题：如何构建决策树（特征选择与剪枝）

决策树构建的核心是两个问题：① 每次分支选择哪个特征？（特征选择）；② 何时停止分支？（剪枝，避免过拟合）。

#### （1）特征选择：基于“纯度提升”的准则

特征选择的目标是选择能让分支后样本“纯度”最高的特征（即子集内样本尽可能同类）。常用纯度衡量指标有3种，对应不同的决策树算法：

1. **信息增益（ID3算法）**：

2. **信息增益比（C4.5算法）**：

3. **基尼系数（CART算法）**：

#### （2）剪枝：解决过拟合问题

决策树若不限制分支深度，会不断分裂直至叶节点仅含1个样本，导致“过拟合”（训练误差极低，测试误差极高）。剪枝是通过移除部分分支简化模型，提升泛化能力，分为两种方式：

1. **预剪枝（Pre-pruning）**：

2. **后剪枝（Post-pruning）**：

### 4.  分类树与回归树的区别

|对比维度|分类树（用于分类任务）|回归树（用于回归任务）|
|---|---|---|
|叶节点输出|对应类别（如“正类/负类”）|对应数值（如“房价预测值”）|
|纯度/误差指标|信息增益、信息增益比、基尼系数|均方误差（MSE）、平均绝对误差（MAE）|
|分支准则|选择纯度提升最大的特征|选择分支后MSE/MAE最小的特征|
### 5.  核心考点与易错点


      1.  三种决策树算法的核心区别：ID3用信息增益（偏向多取值特征），C4.5用信息增益比（修正偏向），CART用基尼系数（高效，支持分类+回归）。
      2.  决策树的优缺点：优点是可解释性强、无需预处理、捕捉特征交互；缺点是易过拟合、对噪声数据敏感、预测稳定性差（微小数据变化可能导致树结构巨变）。
      3.  连续型特征的处理：CART树通过遍历所有可能的阈值，将连续特征离散化为“≤阈值”和“>阈值”两类，再计算纯度/误差指标选择最优阈值。
      4.  剪枝的核心目的：解决过拟合，预剪枝高效但可能欠拟合，后剪枝泛化能力强但计算复杂。
    

## 二、集成学习（Ensemble Learning）

### 1.  算法定位与核心思想

集成学习并非单一算法，而是一种“组合策略”：通过训练多个基础模型（称为“基学习器”，如决策树、逻辑回归），再将它们的预测结果组合起来，得到最终预测。核心思想是“集思广益”——单个基学习器可能存在偏差或方差，但多个基学习器的组合可降低泛化误差，提升模型稳定性和性能。

核心假设：① 基学习器具有多样性（预测误差不完全相关）；② 基学习器的性能优于随机猜测（即“弱学习器”，如深度较浅的决策树）。

常见集成策略：Bagging（并行式集成，降低方差）、Boosting（串行式集成，降低偏差）、Stacking（堆叠集成，组合不同类型基学习器）。

### 2.  Bagging与随机森林（Random Forest）

#### （1）Bagging的核心逻辑

Bagging（Bootstrap Aggregating）是“并行式集成”的代表：

1. 采样：通过“自助采样”（Bootstrap）从原始训练集中随机抽取 \( m \) 个样本（有放回采样，即同一样本可被多次选中），生成 \( T \) 个不同的采样集（每个采样集对应一个基学习器）。

2. 训练：并行训练 \( T \) 个基学习器（每个基学习器独立训练，无依赖关系）。

3. 组合：分类任务采用“投票法”（少数服从多数），回归任务采用“平均法”（取所有基学习器预测值的均值）。

核心作用：降低单个基学习器的方差（避免过拟合），提升模型稳定性。因为自助采样引入了样本多样性，多个基学习器的误差相互抵消，组合后泛化能力更强。

#### （2）随机森林：Bagging的改进与实现

随机森林是以“决策树”为基学习器的Bagging集成算法，核心改进是在“样本采样”的基础上增加“特征采样”，进一步提升基学习器的多样性：

1. 双重采样：① 样本自助采样（生成 \( T \) 个采样集）；② 每个基学习器训练时，从 \( n \) 个特征中随机选择 \( k \) 个特征（\( k \ll n \)，通常取 \( k = \sqrt{n} \)），仅用这 \( k \) 个特征构建决策树。

2. 训练：并行训练 \( T \) 个决策树（基学习器），每个决策树可深度较大（无需严格剪枝，因为多样性采样已降低过拟合风险）。

3. 组合：分类用投票法，回归用平均法。

优势：① 性能优于单一决策树和传统Bagging（双重采样提升多样性）；② 训练效率高（并行训练）；③ 可评估特征重要性（通过特征在所有决策树中的分裂贡献度计算）；④ 对噪声数据和过拟合的抵抗力强。

关键超参数：① 基学习器数量 \( T \)（通常越大性能越好，直至收敛）；② 每个基学习器的特征数量 \( k \)；③ 决策树的最大深度、最小样本分裂数等。

### 3.  Boosting与梯度提升树（GBDT）

#### （1）Boosting的核心逻辑

Boosting是“串行式集成”的代表，与Bagging的并行训练不同，Boosting的基学习器是“串行训练”的，后续基学习器会重点关注前序基学习器预测错误的样本（即“纠正错误”）：

1. 初始化：给所有训练样本分配相同的权重。

2. 串行训练：训练第 \( t \) 个基学习器后，根据其预测误差调整样本权重——预测错误的样本权重增加，预测正确的样本权重降低。

3. 组合：每个基学习器有不同的“可信度权重”（预测误差越小，权重越大），最终预测结果是所有基学习器预测值的加权和。

核心作用：降低单个基学习器的偏差（提升模型拟合能力），适合解决欠拟合问题。常见Boosting算法：AdaBoost（自适应提升）、GBDT（梯度提升决策树）、XGBoost、LightGBM。

#### （2）GBDT的核心原理

GBDT（Gradient Boosting Decision Tree）是以“回归树”为基学习器的Boosting算法，核心改进是用“梯度下降”的思想替代AdaBoost的权重调整，更通用、更高效：

1. 初始化：用一个常数（如所有样本标签的均值）作为初始预测值。

2. 串行训练（核心步骤）：

3. 终止条件：当基学习器数量达到阈值，或模型在验证集上的误差不再下降时停止。

关键说明：① GBDT的基学习器必须是回归树（即使处理分类任务，也需将标签转换为概率，用回归树拟合）；② 学习率 \( \eta \)（通常取0.01~0.1）是核心超参数，过小需更多基学习器，过大会导致过拟合；③ GBDT可处理分类、回归、排序等多种任务，只需更换对应的损失函数（分类用交叉熵，回归用MSE）。

#### （3）XGBoost与LightGBM：GBDT的工程优化

XGBoost（Extreme Gradient Boosting）和LightGBM是GBDT的工业级优化版本，核心优势是训练速度快、泛化能力更强，是竞赛和工业界的主流算法：

1. **XGBoost的核心优化**：① 加入正则化项（惩罚基学习器的复杂度，避免过拟合）；② 支持并行计算（特征分裂时的阈值遍历可并行）；③ 处理缺失值（自动学习缺失值的分裂方向）；④ 采用二阶泰勒展开（加速梯度下降收敛）。

2. **LightGBM的核心优化**：① 直方图优化（将连续特征离散化为直方图，减少计算量）；② 单边梯度采样（采样梯度大的样本，提升训练效率）；③ 叶子生长策略（从当前叶子扩展，而非层序生长，减少冗余节点）；④ 并行优化（样本、特征并行），训练速度远快于XGBoost。

### 4.  Bagging与Boosting的核心区别

|对比维度|Bagging（如随机森林）|Boosting（如GBDT、XGBoost）|
|---|---|---|
|训练方式|并行训练（基学习器独立，无依赖）|串行训练（基学习器依赖，后续纠正前序误差）|
|样本采样|自助采样（有放回，样本多样性）|无采样，通过调整样本权重聚焦错误样本|
|核心作用|降低方差（避免过拟合，提升稳定性）|降低偏差（提升拟合能力，解决欠拟合）|
|基学习器要求|基学习器可复杂（如深度大的决策树）|基学习器需简单（如深度浅的决策树，称为“弱学习器”）|
|过拟合风险|对基学习器数量不敏感，过拟合风险低|基学习器数量过多易过拟合，需调学习率和正则化|
### 5.  核心考点与易错点


      1.  集成学习的核心逻辑：“多样性+弱学习器组合”，通过组合降低泛化误差，Bagging降方差，Boosting降偏差。
      2.  随机森林的双重采样：样本自助采样+特征随机选择，二者共同提升基学习器多样性，避免过拟合。
      3.  GBDT的核心误区：GBDT是“梯度提升”而非“梯度下降”，基学习器是回归树，通过拟合残差（负梯度）逐步纠正误差；学习率越小，需要的基学习器数量越多，模型越稳定。
      4.  XGBoost与LightGBM的优势：二者均是GBDT的优化版本，核心差异在工程实现（并行、直方图等），LightGBM速度更快，适合大数据；XGBoost泛化能力强，适合小数据集竞赛。
      5.  集成学习的适用场景：需要高性能模型的工业场景（如推荐系统、风控）、机器学习竞赛，缺点是模型可解释性差（“黑箱”）、训练和部署成本较高。
    