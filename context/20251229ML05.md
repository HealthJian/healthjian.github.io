# 机器学习复习回忆资料（第五篇：模型评估与选择）

说明：模型训练完成后，需通过科学的评估指标判断模型性能，再基于评估结果选择最优模型。不同类型的任务（监督/无监督）、不同的数据分布（如类别不平衡），适用的评估指标和选择方法不同。本篇将系统梳理监督学习（回归/分类）和无监督学习（聚类）的核心评估指标，同时讲解模型选择的关键方法（如交叉验证、超参数调优），为模型落地应用提供核心依据。

## 一、监督学习模型评估（核心：依赖标签的评估）

监督学习分为回归和分类两大任务，两类任务的输出类型不同（连续值/离散标签），评估指标差异较大，需分别掌握。

### 1.  回归任务评估指标

回归任务的核心是衡量模型预测的连续值与真实值的误差，常用指标如下：

#### （1）均方误差（Mean Squared Error, MSE）

· 定义：预测值与真实值差值的平方和的平均值，是回归任务最基础的评估指标。

· 公式：\( MSE = \frac{1}{m} \sum_{i=1}^m (\hat{y}_i - y_i)^2 \)

· 说明：\( \hat{y}_i \) 是第i个样本的预测值，\( y_i \) 是真实值，m是样本数量。MSE对异常值敏感（平方项会放大异常值的误差），值越小说明模型预测越准确。

· 衍生指标：均方根误差（RMSE），即MSE的平方根，\( RMSE = \sqrt{MSE} \)。优势是量纲与原始标签一致（如房价预测中，RMSE单位为“元”），更易理解实际误差大小。

#### （2）平均绝对误差（Mean Absolute Error, MAE）

· 定义：预测值与真实值差值的绝对值的平均值。

· 公式：\( MAE = \frac{1}{m} \sum_{i=1}^m |\hat{y}_i - y_i| \)

· 说明：MAE对异常值的鲁棒性强（无平方项），但数学性质不如MSE（MAE在原点不可导，优化难度略高）。当数据中存在较多异常值时，优先选择MAE。

#### （3）决定系数（Coefficient of Determination, R²）

· 定义：衡量模型解释数据变异的能力，即“模型预测值能解释的方差占真实值总方差的比例”，是回归任务最常用的综合指标。

· 公式：\( R^2 = 1 - \frac{\sum_{i=1}^m (\hat{y}_i - y_i)^2}{\sum_{i=1}^m (y_i - \bar{y})^2} \)，其中 \( \bar{y} = \frac{1}{m} \sum_{i=1}^m y_i \) 是真实值的均值。

· 说明：① R²取值范围为(-∞, 1]；② R²=1时，模型完美预测所有样本；③ R²=0时，模型预测效果等同于直接预测均值；④ R²<0时，模型预测效果差于均值预测（通常是模型选择错误或数据预处理不当）。

· 注意：调整R²（Adjusted R²）：当特征数量增加时，R²会自动增大（即使新增特征无意义），调整R²通过惩罚特征数量解决这一问题，更适合多特征回归模型的评估。

### 2.  分类任务评估指标

分类任务的核心是衡量模型预测的离散标签与真实标签的匹配程度，需先理解“混淆矩阵”，再衍生出各类评估指标。

#### （1）基础：混淆矩阵（Confusion Matrix）

· 定义：将分类结果划分为4类，构建2×2矩阵（二分类场景），清晰展示模型的预测正确/错误情况。

· 核心元素（二分类）：

- 真正例（True Positive, TP）：真实标签为正类，预测标签为正类（预测正确）；

- 假正例（False Positive, FP）：真实标签为负类，预测标签为正类（预测错误，“误判为正”）；

- 真负例（True Negative, TN）：真实标签为负类，预测标签为负类（预测正确）；

- 假负例（False Negative, FN）：真实标签为正类，预测标签为负类（预测错误，“漏判为负”）。

· 扩展：多分类场景下，混淆矩阵为n×n矩阵（n为类别数），对角线元素为各类别的正确预测数，非对角线元素为错误预测数。

#### （2）核心评估指标（基于混淆矩阵）

1. **准确率（Accuracy）**：
        · 定义：所有样本中预测正确的比例，是最直观的分类指标。· 公式：\( Accuracy = \frac{TP + TN}{TP + FP + TN + FN} \)· 缺点：不适用于类别不平衡数据（如疾病诊断中，患病样本仅占1%，模型全部预测为负类，准确率仍为99%，但无实际意义）。

2. **精确率（Precision，查准率）**：
        · 定义：预测为正类的样本中，真实为正类的比例（关注“预测为正的样本是否可靠”）。· 公式：\( Precision = \frac{TP}{TP + FP} \)· 适用场景：需避免“误判为正”的场景（如垃圾邮件识别，避免将正常邮件误判为垃圾邮件）。

3. **召回率（Recall，查全率）**：
        · 定义：真实为正类的样本中，被预测为正类的比例（关注“真实正类是否被全部找出”）。· 公式：\( Recall = \frac{TP}{TP + FN} \)· 适用场景：需避免“漏判为负”的场景（如疾病诊断，避免遗漏患病患者）。

4. **F1分数（F1-Score）**：
        · 定义：精确率和召回率的调和平均数，综合衡量二者性能（解决精确率和召回率的矛盾：精确率升高时，召回率通常降低，反之亦然）。· 公式：\( F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall} \)· 说明：F1取值范围为[0,1]，值越大说明模型综合性能越好，适用于类别不平衡数据。

5. **特异度（Specificity）**：
        · 定义：真实为负类的样本中，被预测为负类的比例（关注“真实负类是否被全部找出”）。· 公式：\( Specificity = \frac{TN}{TN + FP} \)· 衍生：假正例率（FPR）= 1 - Specificity，是ROC曲线的核心指标。

#### （3）进阶指标：ROC曲线与AUC值

1. **ROC曲线（Receiver Operating Characteristic Curve）**：
        · 定义：以“假正例率（FPR）”为横轴，“召回率（Recall，也称为真正例率TPR）”为纵轴绘制的曲线，描述模型在不同阈值下的分类性能。· 核心逻辑：通过调整分类阈值（如逻辑回归中，将“概率≥0.5”改为“概率≥0.3”），得到多组FPR和TPR，连接这些点形成ROC曲线。· 曲线解读：① 理想模型的ROC曲线经过(0,1)点（FPR=0，TPR=1），即无假正例、无假负例；② 随机猜测模型的ROC曲线是对角线（y=x），AUC=0.5；③ 模型性能越好，ROC曲线越靠近左上角。

2. **AUC值（Area Under ROC Curve）**：
        · 定义：ROC曲线下的面积，量化ROC曲线的性能，是分类任务最常用的综合指标之一。· 取值范围：[0.5, 1]，AUC越接近1，模型分类性能越好；AUC=0.5时，模型性能等同于随机猜测；AUC<0.5时，模型性能差于随机猜测（可反转预测结果改善）。· 优势：① 对类别不平衡数据不敏感；② 不依赖具体的分类阈值；③ 可用于比较不同模型的性能。

### 3.  分类任务特殊场景：类别不平衡的评估

· 定义：数据集中某一类样本数量远多于另一类（如正类占1%，负类占99%）。

· 不适用指标：准确率（易被多数类主导，失去意义）。

· 推荐指标：① 精确率、召回率、F1分数；② AUC值；③ 混淆矩阵（直接观察少数类的预测情况）。

· 补充：PR曲线（Precision-Recall Curve）：以精确率为纵轴、召回率为横轴，比ROC曲线更适合类别极度不平衡的场景（此时ROC曲线接近左上角，难以区分模型性能，而PR曲线差异更明显）。

## 二、无监督学习模型评估（核心：不依赖标签的评估）

无监督学习无真实标签，评估核心是衡量“聚类结果的合理性”（簇内紧凑、簇间分离），常用指标如下（以聚类任务为例）：

### 1.  内部评估指标（仅基于数据本身，无需标签）

1. **簇内平方和（WCSS/Inertia）**：
       · 定义：所有样本到其所属簇中心的欧氏距离平方和（K-Means的目标函数）。· 解读：WCSS越小，说明簇内样本越紧凑；但需注意，WCSS会随簇数K的增加而单调递减（K越大，簇内样本越少，距离和越小），因此不能单独用于比较不同K的模型性能（需结合肘部法则）。

2. **轮廓系数（Silhouette Coefficient）**：
        · 定义：衡量单个样本的“聚类合理性”，综合考虑“样本与所属簇内其他样本的相似度（a）”和“样本与最近其他簇内样本的相似度（b）”。· 公式：单个样本轮廓系数 \( s = \frac{b - a}{\max(a, b)} \)；整体轮廓系数 = 所有样本轮廓系数的平均值。· 解读：① s取值范围为[-1, 1]；② s接近1：样本聚类合理（与所属簇相似度高，与其他簇相似度低）；③ s接近0：样本处于两个簇的边界，聚类模糊；④ s接近-1：样本聚类错误（应属于其他簇）。整体轮廓系数越接近1，聚类效果越好。

3. **戴维森堡丁指数（Davies-Bouldin Index, DBI）**：
        · 定义：衡量“簇内紧凑性”与“簇间分离度”的比值，值越小越好。· 核心逻辑：计算每个簇的“簇内平均距离”（反映紧凑性）和“与其他簇的簇间距离”（反映分离度），DBI是所有簇的“（簇内平均距离+最近簇的簇内平均距离）/簇间距离”的平均值。· 优势：不受簇数K的影响，可直接比较不同K的聚类模型性能（DBI越小，聚类效果越好）。

### 2.  外部评估指标（需依赖真实标签，适用于有标签的无监督任务验证）

若聚类任务有真实类别标签（如已知用户真实分组，用K-Means验证聚类效果），可使用以下指标：

1. **调整兰德指数（Adjusted Rand Index, ARI）**：
        · 定义：衡量聚类结果与真实标签的“吻合程度”，修正了随机聚类的影响。· 解读：取值范围为[-1, 1]；ARI=1时，聚类结果与真实标签完全一致；ARI=0时，聚类结果等同于随机聚类；ARI<0时，聚类结果差于随机聚类。

2. **互信息（Mutual Information, MI）与调整互信息（Adjusted Mutual Information, AMI）**：
        · 定义：衡量聚类结果与真实标签之间的“信息重叠程度”，MI越大说明吻合度越高，但MI会随簇数增加而增大；AMI修正了这一问题，更适合比较不同模型。· 解读：AMI取值范围为[-1, 1]，越接近1说明聚类结果与真实标签越吻合。

## 三、模型选择的核心方法

模型评估的最终目的是选择“泛化能力最强”的模型（即对未知数据的预测性能最好），核心方法包括数据划分、交叉验证和超参数调优。

### 1.  基础：数据划分（训练集、验证集、测试集）

1. **划分目的**：
       · 训练集（60%~80%）：用于模型参数训练；· 验证集（10%~20%）：用于模型选择和超参数调优（如选择K-Means的K、决策树的最大深度）；· 测试集（10%~20%）：用于最终评估模型的泛化能力（测试集仅能使用一次，避免因多次调整模型导致的过拟合）。

2. **关键原则**：
        · 三个数据集必须独立同分布（i.i.d.），确保评估结果真实反映泛化能力；· 类别不平衡数据需采用“分层划分”（如测试集中正类比例与原始数据一致），避免随机划分导致某类样本在测试集中缺失。

### 2.  核心：交叉验证（Cross-Validation, CV）

· 适用场景：当数据量较小时，单独划分验证集会导致训练数据不足，交叉验证通过“重复划分-训练-评估”解决这一问题。

· 常用类型：K折交叉验证（最常用）：

1. 将训练集随机划分为K个大小相等的子集（通常K=5或10）；

2. 用其中K-1个子集作为训练数据，1个子集作为验证数据，训练并评估模型；

3. 重复K次，确保每个子集都作为一次验证集；

4. 最终模型性能为K次评估结果的平均值（如平均准确率、平均F1分数）。

· 扩展：

- 分层K折交叉验证：适用于类别不平衡数据，确保每折中子集的类别比例与原始数据一致；

- 留一交叉验证（LOOCV）：K=m（m为样本数量），每折仅用1个样本作为验证集，评估结果稳定但计算复杂度极高（适用于极小数据集）。

### 3.  关键：超参数调优

· 定义：模型参数分为“可学习参数”（如线性回归的权重w，由训练数据学习得到）和“超参数”（如K-Means的K、决策树的最大深度，需手动指定），超参数调优即寻找最优的超参数组合。

· 常用方法：

1. **网格搜索（Grid Search）**：
        · 逻辑：为每个超参数指定一组候选值，遍历所有超参数组合，用交叉验证评估每个组合的性能，选择最优组合。· 优点：全面稳定，能找到全局最优组合；缺点：计算复杂度高（候选值越多、超参数越多，耗时越长）。

2. **随机搜索（Random Search）**：
        · 逻辑：为每个超参数指定搜索范围，随机采样多组超参数组合，用交叉验证评估，选择最优组合。· 优点：计算效率高（无需遍历所有组合），实践中性能常接近网格搜索；缺点：可能错过最优组合（需合理设置采样次数）。

3. **贝叶斯优化（Bayesian Optimization）**：
        · 逻辑：基于先验评估结果，构建超参数与模型性能的概率模型，预测“最可能找到最优性能”的超参数组合，迭代优化。· 优点：计算效率极高（无需大量采样），适用于超参数多、候选范围广的场景；缺点：实现复杂度高。

## 四、核心考点与易错点


      1.  评估指标与任务匹配：回归任务用MSE、MAE、R²；分类任务用准确率、精确率、召回率、F1、AUC；聚类任务用轮廓系数、DBI、ARI。
      2.  类别不平衡的坑：避免用准确率评估不平衡数据，优先选择F1、AUC或PR曲线。
      3.  交叉验证的意义：解决小数据量下训练集不足的问题，使模型评估结果更稳定；类别不平衡数据需用分层K折。
      4.  超参数与可学习参数的区别：超参数需手动指定（如K、学习率），可学习参数由训练得到（如权重w）；超参数调优必须用验证集，不可用测试集（避免数据泄露）。
      5.  数据泄露的风险：模型训练/评估过程中若使用了测试集信息（如用测试集的均值标准化），会导致评估结果虚高，泛化能力下降。核心避免方法：所有数据预处理（标准化、缺失值填充）仅基于训练集，验证集和测试集复用训练集的预处理参数。
    

## 五、总结：模型评估与选择流程

1.  数据划分：将原始数据分为训练集、验证集、测试集（分层划分，确保独立同分布）；

2.  模型训练：用训练集训练多个候选模型（不同算法或不同超参数）；

3.  模型评估：用验证集+交叉验证评估候选模型性能（选择匹配任务的评估指标）；

4.  超参数调优：通过网格搜索/随机搜索等方法，优化最优模型的超参数；

5.  最终验证：用测试集评估最优模型的泛化能力，确认模型性能；

6.  模型落地：若测试集性能满足需求，部署模型；否则返回步骤2，重新选择模型或优化数据预处理。
